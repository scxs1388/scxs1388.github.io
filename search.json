[{"title":"Hello Hexo","url":"/2021/09/01/hello-hexo/","content":"总结一下在 GitHub Pages 用 Hexo 框架建站的流程以及过程中踩过的坑。\n\nNexT 主题\n更换 NexT 主题\nNexT 是 Hexo 最受欢迎的主题之一，支持多种样式和配置。\n打开任意命令行解释器，在 Blog 的根目录执行\ngit clone git@github.com:theme-next/hexo-theme-next.git themes/next\n或者到 NexT 主题的 GitHub 仓库下载正式发行的压缩包，解压到 themes/next 目录中。\n之后编辑 _config.yml 文件中的 themes 配置。\ntheme: next\n修改 NexT 主题模板\n编辑 themes/next/_config.yml 文件中的 scheme 配置。\nscheme: Gemini\n修改亮色/暗色模式\n编辑 themes/next/_config.yml 文件中的 darkmode 配置。\ndarkmode: true\n网站个人信息配置\n网站基本信息\n编辑 _config.yml 文件。可以修改主标题、副标题、简介、关键词、作者名称、语言、时区、访问 URL 等配置。\n# Sitetitle: scxs1388&#x27;s Blog # 网站主标题subtitle: Everything happens for the best # 网站副标题description: Falling into singularity # 简介keywords: # 关键词 (搜索用)author: scxs1388 # 作者名称language: zh-CN # 语言timezone: Asia/Shanghai # 时区# URLurl: https://scxs1388.github.io # 访问的 url# Writinghighlight: # 代码高亮  enable: true  line_number: true  auto_detect: true\n网站 Favicon\n配置网站的浏览器标签页图标。\n编辑 themes/next/_config.yml 文件中的 favicon 配置。\n可以设置为在线或本地图片，默认本地图片存储目录为 themes/next/source/images。如果选择使用本地图片，在网站部署后的图片存储目录为 /images，在编辑配置时需要注意。\n头像 Avatar\n侧边栏添加个人头像。\n编辑 themes/next/_config.yml 文件中的 avatar 配置。\navatar:  url: /images/avatar.jpg # 本地图片路径  rounded: true # 圆形边框  rotated: false # 光标hover可旋转，不建议\n社交账号链接\n导航栏/侧边栏添加个人社交账号链接。\n编辑 themes/next/_config.yml 文件中的 social 配置。\nsocial:  GitHub: https://github.com/scxs1388 || fab fa-github  E-Mail: mailto:scxs1388@gmail.com || fa fa-envelope\nCreative Commons\n配置网站的 CC 协议。\n编辑 themes/next/_config.yml 文件中的 creative_commons 配置。\ncreative_commons:  license: by-nc-sa ## cc协议  size: small # 大小  sidebar: true # 侧边栏显示  post: true # 文章结尾显示\n网站页面配置\n导航栏/侧边栏\n配置导航栏/侧边栏的栏目和外观。\n编辑 themes/next/_config.yml 文件中的 menu 配置。\nmenu:  home: / || fa fa-home # 主页  about: /about/ || fa fa-user # 关于  tags: /tags/ || fa fa-tags # 文章 tag  categories: /categories/ || fa fa-th # 文章类别  archives: /archives/ || fa fa-archive # 文章归档menu_settings:  icons: true # 显示icon  badges: true # 显示相应分类的数量\n网页尾栏\n更改网页尾栏的信息。\n编辑 themes/next/_config.yml 文件中的 footer 配置。\nfooter:  since: 2021 # 网站建立时间  icon:    name: fa fa-heart    animated: true    color: &quot;#ff0000&quot;  powered: true\nGitHub 标识\n设置网站右上角的 GitHub 标识。\n编辑 themes/next/_config.yml 文件中的 github_banner 配置。\ngithub_banner:  enable: true  permalink: https://github.com/scxs1388  title: Follow me on GitHub\nTag icon\n为文章底部的每个 Tags 前添加一个 icon。\n编辑 themes/next/_config.yml 文件中的 tag_icon 配置。\ntag_icon: true\n各种字体\n设置网站的字体。包括文字样式，文字大小，在线字体 host 等。\n编辑 themes/next/_config.yml 文件中的 font 配置。\nfont:  enable: true  host: https://fonts.font.im  global: # 全局字体 (指 &lt;body&gt; 内的所有元素)    external: true # 设为 true 表示启用在线字体 host    family: Lato    size: 0.84  title: # 网站主标题的字体    external: true    family: Roboto Slab    size:  headings: # 标题字体 (&lt;h1&gt; 到 &lt;h6&gt;)    external: true    family: Roboto    size:  posts: # 文章字体    external: true    family: Roboto  codes: # 代码块字体    external: true    family: Input Mono  # Source Code Pro  # JetBrains Mono\n页面加载进度条\n启用页面加载时的进度条显示。\n编辑 themes/next/_config.yml 文件中的 pace 配置。\npace:  enable: true  theme: minimal\n注意，有时会出现 pace.min.css 和 pace.min.js 文件缺失的情况。此时需要在 themes/next/_config.yml 文件的 vendors 配置中设置CDN的地址，或者手动添加这两个文件到 themes/next/source/lib/pace 目录中。\n文章功能配置\n公式 MathJax\n启用 MathJax 编写公式。\n编辑 themes/next/_config.yml 文件中的 math 配置。\nmath:  mathjax:    enable: true # 一般用 mathjax 足够了  katex:    enable: true # 可以设成 false\n高级 Markdown 渲染\n启用 Markdown-it 作为渲染引擎来解析 Markdown 文本。\n打开命令行，在 Blog 的根目录执行\nnpm uninstall hexo-renderer-marked --savenpm install hexo-renderer-markdown-it --save\n编辑 _config.yml 文件，添加 markdown 配置。\nmarkdown:  renderer: markdown-it\n图片浏览插件 Fancybox\n查看文章图片时使用 FancyBox 插件。\n编辑 themes/next/_config.yml 文件中的 fancybox 配置。\nfancybox: true\n字数统计插件\n打开命令行，在 Blog 的根目录执行\nnpm install hexo-word-counter --savehexo clean\n编辑 _config.yml 文件，添加 symbol_count_time 配置。\nsymbols_count_time:  symbols: true  time: true  total_symbols: true  total_time: true  exclude_codeblock: false  wpm: 200  suffix: &quot;mins&quot;\n评论插件 Valine (不建议使用)\n添加 Valine 评论系统。Valine 是基于 LeanCloud 云服务的前端评论系统，要启用 Valine 需要事先在 LeanCloud 注册并创建一个应用获取 Appid 和 AppKey。\n上述步骤完成之后，编辑 themes/next/_config.yml 文件中的 valine 配置。\nvaline:  enable: true  appid:  # Your leancloud application appid  appkey: # Your leancloud application appkey  notify: false # Mail notifier  verify: false # Verification code  placeholder: Just go go # Comment box placeholder  avatar: mm # Gravatar style  guest_info: nick,mail,link # Custom comment header  pageSize: 10 # Pagination size  language: zh-cn # Language, available values: en, zh-cn  visitor: true # Article reading statistic  comment_count: true # If false, comment count will only be displayed in post page, not in home page  recordIP: false # Whether to record the commenter IP\n由于 Valine 评论系统完全基于 JavaScript 前端实现，评论内容不会受到服务器端的验证，因此可能会存在垃圾评论的情况。现在 Valine 已经不再建议使用，可以转而使用 Waline 等其他评论系统。\n站内搜索\n启用站内搜索需要安装hexo-generator-searchdb 插件。\n打开命令行，在 Blog 的根目录执行\nnpm install hexo-generator-searchdb --save\n之后编辑 themes/next/_config.yml 文件中的 local_search 配置。\nlocal_search:  enable: true\n阅读进度条和进度显示\n启用阅读进度条和进度显示。\n编辑 themes/next/_config.yml 文件中的 back2top 和 reading_progress 配置。\nback2top:  enable: true  sidebar: false  scrollpercent: truereading_progress:  enable: true  position: top  color: &quot;#37c6c0&quot;  height: 3px\n文章隐藏\n启用文章隐藏功能可以通过修改文章生成的源代码来实现，为了方便可以安装hexo-hide-posts 插件。\n打开命令行，在 Blog 的根目录执行\nnpm install hexo-hide-posts --save\n之后编辑 _config.yml 文件，添加：\n# hexo-hide-postshide_posts:  enable: true  # Change the filter name to fit your need  filter: hidden  # Generators which you want to expose all posts (include hidden ones) to.  # Common generators: index, tag, category, archive, sitemap, feed, etc.  public_generators: []  # Add &quot;noindex&quot; meta tag to prevent hidden posts from being indexed by search engines  noindex: true\n在写文章时，如果想要隐藏文章，在 front-matter 添加 hidden: true 即可。\n网站美化 (进阶)\n更换背景图片\n更换网站的背景图片。\n编辑 themes/next/source/css/main.styl 文件。添加如下代码：\n// Customized Backgroundbody &#123;    background-image: linear-gradient(rgba(0, 0, 0, 0.85), rgba(0, 0, 0, 0.85)), url(&quot;https://s2.loli.net/2025/06/25/LIZHY5o9aKwVviX.jpg&quot;);    background-repeat: no-repeat;    background-attachment: fixed;    background-size: cover;    background-position: center;&#125;\n需要特别注意：如果使用了 Dark Reader 等浏览器插件，可能会导致自定义的 CSS 样式被覆盖，届时背景图片的样式可能不会正常显示。一些有关的问题在 Dark Reader 的 GitHub 主页上有相关的 issue 讨论，但仍未得到解决。\n如果希望加上背景图随着鼠标移动而移动的效果，可以使用 parallax。忽略上面的代码，编辑 themes/next/source/css/main.styl 文件。添加如下代码：\n.parallax-background &#123;  width: 102%;  height: 102%;  /* z-index: -1; */  background-image: linear-gradient(rgba(0, 0, 0, 0.85), rgba(0, 0, 0, 0.85)), url(&quot;https://s2.loli.net/2025/06/25/LIZHY5o9aKwVviX.jpg&quot;);  background-attachment: fixed;  background-size: cover;  background-position: center;  top: -1%;  left: -1%;  background-repeat: no-repeat;  will-change: transform;  transition: transform 0.1s ease-out;  position: fixed;&#125;\n然后，在 themes/next/layout/_layout.swig 文件中的 &lt;body&gt; 前添加如下 JavaScript 代码：\n&lt;script&gt;/* parallax.js */document.addEventListener(&#x27;DOMContentLoaded&#x27;, () =&gt; &#123;  const bg = document.getElementById(&#x27;parallax-bg&#x27;);  const intensity = 16; // 控制移动幅度（值越大移动越明显）  window.addEventListener(&#x27;mousemove&#x27;, (e) =&gt; &#123;    const x = (e.clientX / window.innerWidth - 0.5) * intensity;    const y = (e.clientY / window.innerHeight - 0.5) * intensity;    bg.style.transform = `translate($&#123;-x&#125;px, $&#123;-y&#125;px)`;  &#125;);&#125;);&lt;/script&gt;\n最后在 themes/next/layout/_layout.swig 文件中的 &lt;body&gt; 标签内添加一个 div 元素作为背景图片容器：\n&lt;div class=&quot;parallax-background&quot; id=&quot;parallax-bg&quot;&gt;&lt;/div&gt;\n更改页面元素透明度\n为了使得背景图片不被侧边栏和文章页面遮挡，需要修改页面元素透明度。\n编辑 themes/next/source/css/main.styl 文件。添加如下代码：\n.header-inner &#123; /* 分栏背景色 */  background-color: rgba(51, 51, 51, 0.75);&#125;.sidebar &#123; /* 侧边栏背景色 */  background-color: rgba(51, 51, 51, 0.0);&#125;.sidebar-inner &#123;  background-color: rgba(51, 51, 51, 0.75);  z-index: 1;&#125;.post-block &#123; /* 文章块背景色 */  background-color: rgba(51, 51, 51, 0.75);&#125;.search-popup &#123; /* 搜索弹窗背景色 */  opacity: 0.9;&#125;\n注意：实际上应当尽量避免使用 opacity 属性，因为它会影响到子元素的透明度。这里使用 rgba 来设置颜色的透明度。\n更改代码块样式\n更改代码块的主题样式。\n编辑 themes/next/_config.yml 文件中的 codeblock 配置。\ncodeblock:  highlight_theme: night  copy_button:    enable: true    show_result: default\n此外，在某些情况下，文本行内的内联代码块样式不会随 NexT 主题的亮色/暗色模式的更改而发生变化。此时需要手动调整内联代码块 &lt;code&gt; 的样式，这里只调整暗色模式下的代码块样式。\n编辑 themes/next/source/css/main.styl 文件。添加如下代码：\ncode &#123;  background: #1d1f21;  color: #ffc5c5;&#125;\n更换滚动条样式\n更换右侧滚动条样式。\n编辑 themes/next/source/css/main.styl 文件。添加如下代码：\n// Customized Scrollbar::-webkit-scrollbar &#123;  width: 6px;  height: 9px;  background-color: #454545;&#125;::-webkit-scrollbar-track &#123;  border-radius: 10px;  background-color: #454545;&#125;::-webkit-scrollbar-thumb &#123;  border-radius: 10px;  background-color: #858585;&#125;::-webkit-scrollbar-thumb:hover &#123;  background-color: #9e9e9e;&#125;\n添加网站建立时间\n在尾栏 footer 添加网站建立时间计时器。\n编辑 themes/next/layout/_partials/footer.swig 文件。\n&#123;%- if theme.footer.powered %&#125;  &lt;div class=&quot;powered-by&quot;&gt;    &#123;%- set next_site = &#x27;https://theme-next.org&#x27; %&#125;    &#123;%- if theme.scheme !== &#x27;Gemini&#x27; %&#125;      &#123;%- set next_site = &#x27;https://&#x27; + theme.scheme | lower + &#x27;.theme-next.org&#x27; %&#125;    &#123;%- endif %&#125;    &#123;&#123;- __(&#x27;footer.powered&#x27;, next_url(&#x27;https://hexo.io&#x27;, &#x27;Hexo&#x27;, &#123;class: &#x27;theme-link&#x27;&#125;) + &#x27; &amp; &#x27; + next_url(next_site, &#x27;NexT.&#x27; + theme.scheme, &#123;class: &#x27;theme-link&#x27;&#125;)) &#125;&#125;    &lt;!-- 从这里开始添加代码 --&gt;    &lt;span class=&quot;post-meta-divider&quot;&gt;|&lt;/span&gt;    &lt;span id=&quot;sitetime&quot; class=&quot;post-meta-item&quot;&gt;&lt;/span&gt;    &lt;script&gt;        function siteTime()&#123;        window.setTimeout(&quot;siteTime()&quot;, 1000);        var seconds = 1000;        var minutes = seconds * 60;        var hours = minutes * 60;        var days = hours * 24;        var years = days * 365;        var today = new Date();        var todayYear = today.getFullYear();        var todayMonth = today.getMonth()+1;        var todayDate = today.getDate();        var todayHour = today.getHours();        var todayMinute = today.getMinutes();        var todaySecond = today.getSeconds();        var t1 = Date.UTC(2021,09,01,00,00,00); // 网站建立时间        var t2 = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);        var diff = t2 - t1;        var diffYears = Math.floor(diff/years);        var diffDays = Math.floor((diff/days)-diffYears*365);        var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);        var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);        var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);        document.getElementById(&quot;sitetime&quot;).innerHTML=&quot; 本站已安全运行 &quot;+diffYears+&quot; years &quot;+diffDays+&quot; days &quot;+diffHours+&quot; hours &quot;+diffMinutes+&quot; mins &quot;+diffSeconds+&quot; secs&quot;;        &#125;        siteTime();    &lt;/script&gt;  &lt;/div&gt;&#123;%- endif %&#125;\nLive2D\nhexo-helper-live2d (不建议使用)\n打开命令行，在 Blog 的根目录执行\nnpm install hexo-helper-live2d --save\n编辑 _config.yml 文件，加入如下代码：\n## Live2dlive2d:  enable: true  scriptFrom: local  pluginRootPath: live2dw/  pluginJsPath: lib/  pluginModelPath: assets/  tagMode: false  debug: false  model:    use: model # 模型名称    scale: 1    hHeadPos: 0.5    vHeadPos: 0.618  display:    superSample: 2    width: 150    height: 300    position: right    hOffset: 15    vOffset: -20  mobile:    show: true    scale: 0.5  react:    opacityDefault: 0.7    opacityOnHover: 0.2\nhexo-helper-live2d 自带部分公开的 Live2D 模型可供使用，具体可以在 https://github.com/xiazeyu/live2d-widget-models 查看和下载。\nhexo-helper-live2d 也支持自定义模型文件。自定义的模型文件需要存放在 Blog 根目录的 live2d_models 目录中，并且也需要修改 _config.yml 配置中相应的模型名称，在生成静态文件时会根据配置进行读取。\nlive2d-widget\n参考项目：在网页中添加Live2D看板娘\n打开命令行，在 Blog 的根目录执行\ngit clone git@github.com:stevenjoezhang/live2d-widget.git themes/next/source/live2d-widget\n编辑 themes/next/source/live2d-widget/autoload.js 文件，修改 live2d_path 为部署后静态文件的相对路径：\nconst live2d_path = &quot;/live2d-widget/dist/&quot;;\n编辑 themes/next/layout/_layout.swig 文件，在 &lt;head&gt; 和 &lt;body&gt; 之间添加如下代码：\n&lt;script src=&quot;/live2d-widget/dist/autoload.js&quot;&gt;&lt;/script&gt;\n编辑 themes/next/_config.yml 文件，添加 live2d 配置。\nlive2d:  enable: true\n本站使用的是非默认的 Live2D 模型，在不使用 CDN 的情况下，一般将模型文件放置在 themes/next/source/live2d-widget/dist/assets 目录中。然后，修改 themes/next/source/live2d-widget/dist/waifu-tips.json 配置文件。\n&#123;  ...  &quot;models&quot;: [&#123;    &quot;name&quot;: &quot;Hutao&quot;,    &quot;paths&quot;: [&quot;/live2d-widget/dist/assets/hutaoyao-live2d/hutao.model3.json&quot;],    &quot;message&quot;: &quot;来自 Genshin Impact 的 胡桃 ～&quot;  &#125;]&#125;\n如果 Live2D 模型的位置未贴合页面最下方，可以通过修改 themes/next/source/live2d-widget/dist/waifu.css 样式文件中的属性，在 #waifu 标签中添加：\n#waifu &#123;    margin-bottom: -80px; /* 向下调整 Live2D 模型位置 */&#125;\n手动调整 Live2D 的模型大小和方向，可以通过修改 #live2d 标签来实现：\n#live2d &#123;    /* cursor: grab; */ /* 取消光标移动到模型时的抓手样式 */    transform: scale(-0.75, 0.75); /* 调整 Live2D 模型大小以及翻转 */&#125;\n当然，这里并没有考虑到鼠标光标移动时与 Live2D 的 Canvas 的交互效果，这个问题以后有时间再解决。\n另外，在使用 live2d-widget 在隐藏后再点击 waifu-toggle 可能会存在显示时图像消失的问题。\n这个问题可以通过修改 themes/next/source/live2d-widget/waifu-tips.js 中的监听事件函数解决。\nlive2d-widget 项目经过重构后，waifu-tips.js 文件已经过混淆处理，一种简便的修改方式为：\n\n在 waifu-tips.js 文件中 Ctrl+F 搜索并定位 3e3；\n将位置左侧附近的 e.classList.add(&quot;waifu-hidden&quot;); 注释掉。\n\n以下是一些 Live2D 模型相关资源的链接：\n\nProject Sekai\n碧蓝航线\n根瘤菌rkzj\n梦象\nsummerscar\nFaceRig workshop\nLive2dViewerEX workshop\n\nHexo 的使用相关\nHexo 常用命令\n# 生成静态文件hexo generate# 生成静态文件并启动本地服务器hexo server# 生成静态文件并部署到远程服务器hexo deploy# 清除 Hexo 缓存hexo clean\n使用 SSH 进行部署\nHexo 默认的 git 使用 HTTPS 协议。由于 GitHub 的 HTTPS 443 端口由于网络连接原因经常出现连接超时的情况，因此建议为 GitHub 配置 SSH keys 方便今后部署到 GitHub Pages。\n这里省略生成 SSH key、本地配置 SSH config 文件、在 GitHub 配置 SSH key 等步骤，只介绍 Hexo 配置 SSH 的步骤。\n启用 SSH 需要安装hexo-deployer-git 插件。打开命令行，在 Blog 的根目录执行\nnpm install hexo-deployer-git --save\n之后编辑 _config.yml 文件。\ndeploy:  type: git # 使用 git 协议  repo: git@github.com:scxs1388/scxs1388.github.io.git  branch: page # 默认 main，如不是需要修改"},{"title":"How to Break MD5 and Other Hash Functions","url":"/2021/12/04/break-md5/","content":"MD5(Message Digest 5) 诞生于 1992 年，是对 MD4 算法的改进。作为目前使用最为普遍的加密哈希函数之一，MD5 自诞生起其安全性就被广泛研究。针对 MD5 最著名的攻击方式是 semi-free start collision，这种攻击方式将哈希函数的参数初始标准值替换为了其它的非标准值，从而实现了攻击。本文提出了一种针对 MD5 的攻击方式，该攻击能够在 $15$ 分钟到一个小时的计算时间内发现 MD5 碰撞。该攻击是一种差分攻击，但与大多数差分攻击不同，该攻击使用模整数减法代替异或作为差异度量，这种差分称为模差 (modular differential)。该攻击在 MD4 上可以在不到几分之一秒的时间内发现碰撞。此外，这种攻击也适用于 RIPEMD、HAVAL 等其他哈希函数。\n\n\n\n\nTitle: How to Break MD5 and Other Hash Functions\nVenue: EUROCRYPT 2005\nAuthor(s): Xiaoyun Wang, Hongbo Yu\nInstitution(s): Shandong University\nLink(s): Springer\n\n\n1 Introduction\n数字签名 (Digital Signatures) 在信息安全领域十分重要。数字签名的安全性取决于其底层实现的哈希函数的强度。哈希函数在密码学中还有如数据完整性 (Data Integrity)、分组签名 (Group Signature)、电子现金 (E-Cash) 等诸多应用。在这些应用中使用哈希函数不仅可以保证安全性，而且也能很大程度上提高效率。目国际通用的哈希函数有 MD5 和 SHA-1 两种。\nMD5 是由 Ron Rivest 设计的哈希函数，是 MD4 的加强版。自 1992 年发布至今，研究者们已经逐渐发现了 MD5 的一些弱点。1993 年，B.den Boer 和 A. Bosselaers 发现了一种 MD5 的由两组相同信息和不同初始值产生的伪碰撞 (Pseudo-Collision)。这种攻击揭露了 MD5 中所有链接变量最高有效位的弱雪崩现象。在 Eurocrypt’96 的 Rump Session，H. Dobbertin 提出了一种 semi free-start collision 攻击，该攻击由两组不同的信息和给定的初始值 $IV_0’$ 组成。$IV_0’$ 取值为：\n$$\na_0=\\text{0x12ac2375},b_0=\\text{0x3b341042},c_0=\\text{0x5f62b97c},d_0=\\text{0x4ba763ed}\n$$\n尽管 H. Dobbertin 的攻击方法并不能得到一个真正的 MD5 碰撞，但这种对 MD5 碰撞的攻击尝试揭露了 MD5 算法的弱雪崩现象。这也使在一次迭代中找到特殊形式的差分成为可能。\n本文介绍将一种非常强大的寻找 MD5 碰撞的攻击方法。受 H. Dobbertin 的攻击方法的启发，我们对以下问题进行研究：是否有可能找到一对信息，每个信息由两个信息块组成，在经过这对信息的第二个信息块后能够产生 MD5 碰撞。具体来说，我们希望找到一对信息 $(M_0,M_1)$ 和 $(M_0’,M_1’)$ 使得：\n$$\n\\begin{align}\n&amp; (a,b,c,d)=\\text{MD5}(a_0,b_0,c_0,d_0,M_0), \\\\\n&amp; (a’,b’,c’,d’)=\\text{MD5}(a_0,b_0,c_0,d_0,M_0’), \\\\\n&amp; \\text{MD5}(a,b,c,d,M_1)=\\text{MD5}(a’,b’,c’,d’,M_1’), \\\\\n\\end{align}\n$$\n其中 $a_0,b_0,c_0,d_0$ 为 MD5 的初始值。以这种方式来寻找 MD5 碰撞比之前的方法效率更高：找到第一个信息块 $(M_0,M_0’)$ 大约需要 $2^{29}$ 次 MD5 运算，找到第二个信息块 $(M_1,M_1’)$ 大约需要 $2^{32}$ 次 MD5 运算。在一台 IBM P690 机器上应用这种攻击找到信息块 $M_0$ 和 $M_0’$ 需要大约一个小时，最快时只需要 $15$ 分钟。之后找到信息块 $M_1$ 和 $M_1’$ 只需要 $15$ 秒到 $5$ 分钟。应用这种攻击方式产生的两个 MD5 碰撞结果 (见表2) 已于 Crypto’04 大会的 Rump Session 中公布。\n在 Crypto’04 大会上 Eli Biham 和 Rafi Chen 介绍了他们对 SHA-0 的近碰撞攻击方法，他们在大会的 Rump Session 中介绍了他们对 SHA-0 和 SHA-1 攻击的最新成果。A. Joux 介绍了对 SHA-0 的 $4$ 信息块完全碰撞攻击方法。\n本文的组织结构如下：\n\n第 2 部分简单介绍 MD5 算法；\n第 3 部分介绍本文的攻击方法的主要思想；\n第 4 部分对本文的攻击方法进行详细介绍；\n第 5 部分对全文进行总结，并讨论本文的攻击方法对其它哈希函数的适用性。\n\n2 Description of MD5\n这里简单地介绍 MD5 算法的计算流程。\nMD5 的哈希函数(Hash Function)。通常一个哈希函数是由一个压缩函数 $X=f(Z)$ 迭代若干次得到的。压缩函数能够将一个长度为 $l$ 比特位的信息块 $Z$ 转换为一个长度为 $s$ $(l&lt;s)$ 比特位的哈希值 $X$，在 MD5 算法中，信息块长度 $l=512$, 哈希值长度 $s=128$。这种建立哈希函数的迭代式结构被称为 Merkle-Damgard 结构。对于一个经过填充补齐后比特位长度为 $512$ 倍数的信息 $M$，其迭代过程如下：\n$$\nH_{i+1}=f(H_i,M_i),\\space 0 \\leq i \\leq t-1\n$$\n其中 $M=(M_0,M_2,\\cdots,M_{t-1})$，迭代的初始值 $H_0=IV_0$。\n由于本文提出的攻击方式的信息(两个信息块)长度已是 $512$ 的倍数，因此填充与否不影响攻击结果。\nMD5 的压缩函数 (Compression Function)。信息 $M$ 的每个长度为 $512$ 比特位的信息块 $M_i$ 都等分为 $16$ 个长度为 $32$ 比特位的字 (Word) $M_i=(m_0,m_1,\\cdots,m_{15})$，对于每一个信息块，压缩函数都会分 $4$ 个阶段(Round)执行，每个阶段进行 $16$ 步循环 (step operation)，每连续 $4$ 步循环的操作如下：\n$$\n\\begin{align}\n&amp; a=b+((a+\\phi_i(b,c,d)+w_i+t_i)\\lll s_i) \\\\\n&amp; d=a+((d+\\phi_{i+1}(a,b,c)+w_{i+1}+t_{i+1})\\lll s_{i+1}) \\\\\n&amp; c=d+((c+\\phi_{i+2}(d,a,b)+w_{i+2}+t_{i+2})\\lll s_{i+2}) \\\\\n&amp; b=c+((b+\\phi_{i+3}(c,d,a)+w_{i+3}+t_{i+3})\\lll s_{i+3}) \\\\\n\\end{align}\n$$\n其中运算符 $+$ 表示模 $2^{32}$ 加法运算。$t_{i+j}$ 和 $s_{i+j}$ $(j=0,1,2,3)$ 表示与循环步数相关的常数。$w_{i+j}$ 表示信息字。$\\lll s_{i+j}$ 表示循环左移 $s_{i+j}$ 位。\n每个阶段都会采用不同的非线性轮函数 (Non-linear Round Function)，如下所示：\n$$\n\\begin{align}\n&amp; \\phi_i(X,Y,Z)=(X \\land Y) \\lor (\\neg X \\land Z), &amp; 0 \\leq i \\leq 15, \\\\\n&amp; \\phi_i(X,Y,Z)=(X \\land Z) \\lor (Y \\land \\neg Z), &amp; 16 \\leq i \\leq 31, \\\\\n&amp; \\phi_i(X,Y,Z)=X \\oplus Y \\oplus Z, &amp; 32 \\leq i \\leq 47, \\\\\n&amp; \\phi_i(X,Y,Z)=Y \\oplus (X \\lor \\neg Z), &amp; 48 \\leq i \\leq 63,\\\\\n\\end{align}\n$$\n其中 $X,Y,Z$ 均为长度为 $32$ 比特位的字。\nMD5 的链接变量 (chaining variables)。MD5 的链接变量 $a,b,c,d$ 初始化如下：\n$$\na=\\text{0x67452301}, b=\\text{0xefcdab89}, c=\\text{0x98badcfe}, d=\\text{0x10325476}\n$$\n逐字相加 (Wordwise Addition)。令 $H_{i-1}=(aa,bb,cc,dd)$ 为前一个信息块的链接变量，在经过 $4$ 个阶段之后，$H_{i-1}$ 与经过一次迭代后的链接变量进行逐字相加后得到压缩函数的输出值 $H_i$。\n3 Differential Attack on MD5\n3.1 The Modular Differential and the XOR Differential\n差分攻击是针对哈希函数和分组密码最重要的分析方法之一。差分攻击，尤其是分组密码中的差分攻击，一般是一种以异或 (XOR) 作为差分的攻击方式。E. Biham 和 A. Shamir 使用差分攻击来分析类似 DES(Data Encryption Standard) 的加密系统的安全性，他们在其研究成果中称差分密码分析是一种分析明文对中的特定差异对生成的密文对差异的影响的方法。\n本文对差分的定义是一种基于整数模减法运算差异的精确差分。本文还使用模特征来描述每个阶段的异或运算差异和整数模减法运算差异。两种差异的结合相比两者各自而言能够提供更多有价值的信息。\n例如，对于某个未知量 $X$，当整数模减法运算值为 $X’-X=2^6$ 时，异或运算值有多种可能情况：\n\n第 $7$ 位存在一个比特位的差异：异或运算值为 $\\text{0x00000040}$。这种情况下 $X’$ 和 $X$ 的第 $7$ 位分别是 $1$ 和 $0$；\n存在两个比特位的差异：即减法运算时存在第 $7$ 位向第 $8$ 位借位，异或运算值为 $\\text{0x000000c0}$。这种情况下 $X’$ 和 $X$ 的第 $7$、$8$ 位分别是 $10$ 和 $01$；\n存在三个比特位的差异：即减法运算时存在第 $7$ 位向第 $8$ 位再向第 $9$ 位借位，异或运算值为 $\\text{0x000001c0}$。这种情况下 $X’$ 和 $X$ 的第 $7$、$8$、$9$ 位分别是 $100$ 和 $011$；\n存在更多比特位的差异：与前面类似，减法运算时存在更多连续的借位，$X’$ 和 $X$ 的二进制形式分别如 $1000\\dots$ 和 $0111\\dots$；\n假如整数模减法的运算值为负值，异或运算值仍然相同，$X’$ 和 $X$ 的取值与上述情况相反，即 $X’$ 和 $X$ 的二进制形式分别如 $0001\\dots$ 和 $1110\\dots$。\n\n为了能够更加清楚地解释本文的攻击方式，本文在表示模差异时结合了其它两种差异，即使用一个正或负整数(对 $2^{32}$ 取模)和异或差异来同时表示。其中异或差异由一个由比特位和其对应的符号所组成元素的序列来表示。序列中 $X$ 对应位的值为 $0$ 标记为无符号，$X$ 对应位的值为 $1$ 标记为负号。例如，对于模减法运算的差 $-2^6$，$[7,8,9,\\cdots,22,-23]$ 表示从第 $7$ 位直到 $23$ 位借位的模减法运算的差 $X’-X=-2^6 \\space (X’&lt;X)$，$X$ 的第 $7$ 到 $22$ 位全部为 $0$，第 $23$ 位为 $1$ ；$X’$ 的第 $7$ 到 $22$ 位全部为 $1$ ，第 $23$ 位为 $0$ 。一个更复杂的例子是 $-1-26+2-2^{27}$，$[1,2,3,4,5,-6,7,8,9,10,11,-12,-24,-25,-26,27,28,29,30,31,-32]$，其中的模减法运算的差由多个 $2$ 的指数(可正可负)幂组成，异或差异根据不同借位则会有多种不同的值。需要注意在模减法运算中，最高位即第 $32$ 位符号位也能够被借位，借位后第 $32$ 位没有负号。\n值得一提的是，之前的一些研究也已经使用模差来对 SHA-0、MD4、RIPEMD 等哈希函数进行分析。但与之前的研究相比，本文设计的攻击方式有以下几点优势：\n\n寻找的信息只包含两个 $512$ 位的信息块，因此该攻击能够在两次迭代后产生碰撞；\n该攻击是一种精确的差分攻击，其特征比使用时更加严格，并且该攻击除了给出差异之外，还能给出具体比特位的值；\n该攻击给出了一组能够确保差分产生的充分条件；\n该攻击使用了一种信息修改技术(Message Modification Technique)，能够大幅提高碰撞发生的概率。\n\n3.2 Differential Attacks on Hash Functions\n两个参数 $X$ 和 $X’$ 的差异(Difference)定义为 $\\varDelta X=X’-X$。对于任意两个长度为 $l$ 且为 $512$ 倍数的信息 $M$ 和 $M’$，$M=(M_0,M_1,\\cdots,M_{k-1})$, $M’=({M_0}‘,{M_1}’,\\cdots,{M_{k-1}}')$，哈希函数的完全差分 (Differential) 定义如下：\n$$\n\\varDelta H_0 \\stackrel{(M_0,M_0’)}{\\longrightarrow}\n\\varDelta H_1 \\stackrel{(M_1,M_1’)}{\\longrightarrow}\n\\varDelta H_2 \\stackrel{(M_2,M_2’)}{\\longrightarrow}\n\\cdots\\cdots\n\\varDelta H_{k-1} \\stackrel{(M_{k-1},M_{k-1}')}{\\longrightarrow}\n\\varDelta H,\n$$\n其中 $\\varDelta H_0$ 为初始的差异值，初值为零。$\\varDelta H$ 为两信息的输出差异值。$\\varDelta H_i = \\varDelta IV_i$ 为第 $i$ 次迭代的输出差异值以及下一次迭代的初始值。\n显然，如果 $\\varDelta H=0$，则 $M$ 和 $M’$ 之间发生了碰撞。能够产生碰撞的差分称为碰撞差分 (Collision Differential)。\n已知 MD5 的哈希函数有 $4$ 个阶段，每个阶段进行 $16$ 步循环操作，对于第 $i$ 次迭代的差分 $\\varDelta H_i \\stackrel{(M_i,M_i’)}{\\longrightarrow} \\varDelta H_{i+1}$，可展开为如下形式：\n$$\n\\varDelta H_1 \\stackrel{P_1}{\\longrightarrow}\n\\varDelta R_{i+1,1} \\stackrel{P_2}{\\longrightarrow}\n\\varDelta R_{i+1,3} \\stackrel{P_3}{\\longrightarrow}\n\\varDelta R_{i+1,1} \\stackrel{P_4}{\\longrightarrow}\n\\varDelta R_{i+1,4}=\\varDelta H_{i+1}\n$$\n各阶段差分变化 $\\varDelta R_{j-1} \\longrightarrow \\varDelta R_j (j=1,2,3,4)$ 及其概率 $P_j$ 可展开为如下差分特征(Differential Characteristics)：\n$$\n\\varDelta R_{j-1} \\stackrel{P_{j1}}{\\longrightarrow}\n\\varDelta X_i \\stackrel{P_{j2}}{\\longrightarrow}\n\\cdots\\cdots \\stackrel{P_{j16}}{\\longrightarrow}\n\\varDelta X_{16} = \\varDelta R_j\n$$\n其中 $\\varDelta X_{t-1} \\stackrel{P_{jt}}{\\longrightarrow} \\varDelta X_t, t=1,2,\\cdots\\cdots,16$ 是第 $j$ 阶段中第 $t$ 个循环的差分特征。\n一次迭代的差分 $\\varDelta H_i \\stackrel{(M_i,M_i’)}{\\longrightarrow} \\varDelta H_{i+1}$ 的概率 $P$ 满足：\n$$\nP \\geq \\prod_{i=1}^4 P_j \\space\\text{and}\\space P_j \\geq \\prod_{t=1}^{16} P_{jt}.\n$$\n3.3 Optimized Collision Differentials for Hash Functions\n在 3.1 节提到，本文的攻击方法采用了一种信息修改技术来提高碰撞发生的概率。根据这种修改技术，可以得到一个粗略的方法来寻找一个更优的哈希函数的差分(包括碰撞差分)。\n信息修改的方式有两种：\n\n\n对任意两个信息块 $(M_i, M_i’)$ 和第一阶段的非零差分\n$$\n\\varDelta H_i \\stackrel{(M_i,M_i’)}{\\longrightarrow}\n\\varDelta R_{i+1,1}.\n$$\n本文的攻击方法可以很容易地修改信息块 $M_i$ 以保证第一阶段的差分发生的概率 $P_1=1$。\n\n\n采用多重信息修改技术，不仅可以保证第一阶段的差分的概率保持为 $1$，而且还能大大提高第二阶段的差分的概率。\n\n\n要想找到更优的哈希函数的差分，最好选定一个信息块差异，使其能够在之后两阶段差分仍然有很高的概率发生。\n4 Differential Attack on MD5\n4.1 Notation\n为了简化讨论，在正式介绍本文的攻击方法之前，首先引入如下数学表示：\n\n$M=(m_0,m_1,\\cdots,m_{15})$ 以及 $M’=(m_0’,m_1’,\\cdots,m_{15}‘)$ 分别表示两个长度为 $512$ 个比特位数的信息块。$\\varDelta M = (\\varDelta m_0,\\varDelta m_1,\\cdots,\\varDelta m_{15})$ 表示两个信息块的差异。其中 $\\varDelta m_i=m_i’-m_i$ 为第 $i$ 个字的差异。\n$a_i,b_i,c_i,d_i$ 分别表示第 $4i-3,4i-2,4i-1,4i$ 步循环中压缩函数的输出，其中 $1 \\leq i \\leq 16$。$a_i’,b_i’,c_i’,d_i’$ 的定义与1类似；\n$a_{i,j},b_{i,j},c_{i,j},d_{i,j}$ 分别表示 $a_i,b_i,c_i,d_i$ 的第 $j$ 位，其中第 $1$ 位为最低有效位，第 $32$ 位为最高有效位。\n$\\phi_{i,j}$ 为第 $i$ 步循环操作中的非线性轮函数 $\\phi$ 输出值的第 $j$ 位。\n$\\varDelta x_{i,j}=x_{i,j}'-x_{i,j}=\\pm 1$ 表示改变 $x_i$ 的第 $j$ 位之后产生的位差。$x_i[j],x_i[-j]$ ($x$ 可以是 $a,b,c,d,\\phi$) 是只改变字 $x_i$ 中第 $j$ 位之后的取值。$x_i[j]$ 由 $x_i$ 的第 $j$ 位从 $0$ 变到 $1$ 时得到，$x_i[-j]$ 由 $x_i$ 的第 $j$ 位从 $1$ 变到 $0$ 时得到。\n$\\varDelta x_i[j_1,j_2,\\cdots,j_l]=x_i[j_1,j_2,\\cdots,j_l]-x_i$ 表示 $x_i$ 的第 $j_1,j_2,\\cdots,j_l$ 位变化时产生的差异。$x_i[\\pm j_1, \\pm j_2,\\cdots,\\pm j_l]$ 由 $x_i$ 的第 $j_1,j_2,\\cdots,j_l$ 位变化得到。$+$ 号(一般可忽略)指当前位的值从 $0$ 变为 $1$，$-$ 号指当前位的值从 $1$ 变为 $0$。\n\n4.2 Collision Differentials for MD5\n本文的攻击可以使用 MD5 原本的初始值 $IV_0$ 来寻找不同的由两个长度 $1024$ 位的信息 $M_0,M_1$ 和 $(M_0’,M_1’)$ 产生的碰撞。攻击中的两次迭代产生的碰撞差分如下：\n$$\n\\varDelta H_0 \\stackrel{(M_0,M_0’)}{\\longrightarrow}\n\\varDelta H_1 \\stackrel{(M_1,M_1’)}{\\longrightarrow}\n\\varDelta H=0\n$$\n其中\n$$\n\\begin{align}\n&amp; \\varDelta M_0=M_0’-M_0=(0,0,0,0,2{31},0,0,0,0,0,0,2,0,0,2^{31},0) \\\\\n&amp; \\varDelta M_1=M_1’-M_1=(0,0,0,0,2{31},0,0,0,0,0,0,-2,0,0,2^{31},0) \\\\\n&amp; \\varDelta H_1=(2{31},2+2{25},2+2{25},2+2^{25}) \\\\\n\\end{align}\n$$\n$M_0$ 和 $M_1$ 的非零项分别在第 $5$、$12$、$15$ 个字(从高往低)。$\\varDelta H_1=(\\varDelta a,\\varDelta b,\\varDelta c,\\varDelta d)$ 为四个链接变量 $(a,b,c,d)$ 在第一次迭代后产生的差异。\n$\\varDelta M_0$ 需要保证第 $3$ 和 $4$ 阶段的差分以较高概率出现，$\\varDelta M_1$ 不仅需要保证第 $3$ 和 $4$ 阶段的以较高概率出现差分，而且还要使产生的输出差异能够被输出差异 $\\varDelta H_1$ 抵消。\n碰撞差分的所有差分特征见表 3 和表 5。两个表的所有列表示的含义相同。这里仅对表 3 进行解释。\n\n第 1 列表示循环步数；\n第 2 列表示处理第一个信息块 $M_0$ 时的每步循环中链接变量的值；\n第 3 列表示处理第一个信息块 $M_0$ 时每步循环中的信息字；\n第 4 列表示每步循环中的循环左移的位数；\n第 5、6 列分别表示 $M_0$ 和 $M_0’$ 的信息字和链接变量之间的差异；\n第 7 列表示 $M_0’$ 的链接变量值。\n\n表中第 6、7 列中的空白项代表信息字或链接变量之间没有差异。信息字和链接变量两者均不存在差异的循环步没有在表中列出。\n4.3 Sufficient Conditions for the Characteristics to Hold\n接下来给出如何推导得到一组能够保证在 MD5 的第 $8$ 步循环(表3)中差分特征成立的充分条件的一个具体例子。其它的充分条件可以类似地推导给出。\nMD5的第 $8$ 步循环中的差分特征为：\n$$\n(\\varDelta c_2,\\varDelta d_2,\\varDelta a_2,\\varDelta b_1,) \\longrightarrow \\varDelta b_2.\n$$\n其中每个链接变量满足下列等式关系：\n$$\n\\begin{align}\n&amp; b_1’=b_1 \\\\\n&amp; a_2’=a_2[7,\\cdots,22,-23] \\\\\n&amp; d_2’=d_2[-7,24,32] \\\\\n&amp; c_2’=c_2[7,8,9,10,11,-12,-24,-25,-26,27,28,29,30,31,32,1,2,3,4,5,-6] \\\\\n&amp; b_2’=b_2[1,16,-17,18,19,20,-21,-24] \\\\\n\\end{align}\n$$\n根据第 $8$ 步循环的压缩函数，可得：\n$$\n\\begin{align}\n&amp; b_2=c_2+((b_1+F(c_2,d_2,a_2)+m_7+t_7)\\lll 22 \\\\\n&amp; b_2’=c_2’+((b_1+F(c_2’,d_2’,a_2’)+m_7’+t_7)\\lll 22 \\\\\n&amp; \\phi_7=F(c_2,d_2,a_2)=(c_2 \\land d_2) \\lor (\\neg c_2 \\land a_2) \\\\\n\\end{align}\n$$\n在上面的压缩函数的操作中，$c_2$ 在等式右端项中出现了两次，为了加以区分，不妨令 $c_2^F$ 指代函数 $F$ 内的 $c_2$，$c_2^{NF}$ 指代函数 $F$ 外的 $c_2$。\n本推导过程基于以下两个事实：\n\n由 $\\varDelta b_1=0$ 且 $\\varDelta m_7=0$，可知 $\\varDelta b_2=\\varDelta c_2^{NF}+(\\varDelta \\phi_7 \\lll 22$)；\n固定函数 $F$ 中的一或两个参数后，可以使函数 $F$ 退化为单一变量函数。\n\n基于上述条件，可以得到一组保证差分特征不变的充分条件。以下根据 $\\varDelta b_2$ 不同位的取值情况来分别讨论：\n\n$\\varDelta b_2$ 中非零位的情况\n$\\text{(a)}$ $d_{2,11}=1, b_{2,1}=0$ 的情况下，可以保证 $b_2$ 的第一位发生变化。\n$\\text{i}.$ 若 $d_{2,11}=\\overline{a_{2,11}}=1$，可知 $\\varDelta \\phi_{7,11}=1$；\n$\\text{ii}.$ 执行 $\\lll 22$ 循环左移操作之后，$\\varDelta \\phi_{7,11}$ 在第 $1$ 位；\n$\\text{iii}.$ 由于 $\\varDelta c_{2,1}^{NF}=0$，故 $\\varDelta b_{2,1}=\\varDelta c_{2,1}^{NF} + \\varDelta \\phi_{7,11}=1$。\n$\\text{(b)}$ $d_{2,26}=\\overline{a_{2,26}}=1,b_{2,16}=0$ 且 $b_{2,17}=1$ 的情况下，可以保证 $b_2$ 的第 $16$、$17$ 位发生变化；\n$\\text{©}$ $d_{2,28}=\\overline{a_{2,28}}=0,b_{2,i}=0,i=18,19,20$ 且 $b_{2,21}=1$ 可以保证 $b_2$ 的第 $18$、$19$、$20$、$21$ 位发生变化；\n$\\text{(d)}$ $d_{2,3}=\\overline{a_{2,3}}=0,b_{2,24}=1$ 的情况下，可以保证 $b_2$ 的第 $24$ 位发生变化。可以通过下式证明：\n\n$$\n\\varDelta c_2^{NF}[-24,-25,-26,27]+(\\varDelta \\phi_7[3] \\lll 22)=2{23}-2=-2^{23}.\n$$\n\n\n$\\varDelta b_2$ 中零位的情况\n$\\text{(a)}$ 当 $c_{2,17}=0$ 时，可以保证 ${c_2’}^{NF}$ 的第 $7$、$12$ 位和 $a_2’$ 的第 $17$ 位发生变化，且 $b_2$ 保持不变。可以通过下式简单地进行证明：\n$$\n\\varDelta c_2^{NF}[7,\\cdots,11,-12]+(\\varDelta \\phi_7[17] \\lll 22)=-26+26=0.\n$$\n$\\text{(b)}$ 当 $d_{2,i}=a_{2,i}$ 时，可以保证 $c_2^F$ 的第 $i$ 为发生变化，且 $b_2$ 保持不变，其中 $i \\in {1,2,4,5,25,27,29,30,31}$；\n$\\text{©}$ 当 $c_{2,i}=1$ 时，可以保证 $a_2$ 的第 $i$ 位发生变化，且 $b_2$ 保持不变，其中 $i \\in {13,14,15,16,18,19,20,21,22,23}$；\n$\\text{(d)}$ 当 $d_{2,6}=\\overline{a_{2,6}}=0$ 时，可以保证 $c_2^F$ 的第 $6$ 位发生变化，且 $b_2$ 保持不变；\n$\\text{(e)}$ 当 $a_{2,32}=1$ 时，可以保证 $c_2^F$ 和 $d_2$ 的第 $32$ 位发生变化，且 $b_2$ 保持不变；\n$\\text{(f)}$ 当 $d_{2,i}=0$ 时，可以保证 $a_2$ 和 $c_2^F$ 的第 $i$ 位发生变化，且 $b_2$ 保持不变，其中 $i \\in {8,9,10}$；\n$\\text{(g)}$ 当 $d_{2,12}=1$ 时，可以保证 $a_2$ 和 $c_2^F$ 的第 $12$ 位发生变化，且 $b_2$ 保持不变；\n$\\text{(h)}$ 当 $a_{2,24}=0$ 时，可以保证 $c_2^F$ 和 $d_2$ 的第 $24$ 位发生变化，且 $b_2$ 保持不变；\n$\\text{(i)}$ 当 $c_2^F,d_2,a_2$ 的第 $7$ 位发生变化时，$b_2$ 保持不变。\n\n\n通过类似的方法，可以得出一组充分条件来保证碰撞差分中的所有差分特征均保持不变。\n4.4 Message Modification\n单一信息修改 (Single-message Modification)。为了使攻击更加高效，可以通过固定一些信息字来提前满足一些充分条件，从而对本文的攻击方法加以改进。经过观察发现，很容易就能够生成满足 MD5 前 $16$ 步循环的条件的信息。\n对于每个信息块 $M_0$ (或 $M_1$) 和中间值 ($H_0$ 或第二个信息块的 $H_1$ 和 $H_1’$)，可以采用以下流程来对 $M_0$ (或相应的 $M_1$) 进行修改，使得表 4 和表 6 中的第一阶段 (前 $16$ 步循环) 的条件成立。很容易修改 $M_0$ 使得表 4 中第一阶段的条件成立的概率达到 $1$。\n例如，为了确保表 4 中 $c_1$ 的 $3$ 个条件成立，可以将 $M_0$ 中的 $m_2$ 修改如下：\n$$\nc_1^{new} \\longleftarrow c_1{old}-c_{1,7} \\cdot 26-c_{1,12} \\cdot 2{11}-c_{1,20} \\cdot 2^{19}\n$$\n$$\nm_2^{new} \\longleftarrow ((c_1{new}-c_1) \\ggg 17)+m_2^{old}.\n$$\n通过对 $M_0$ 的每个信息字进行修改，表4中第一阶段的所有条件都能够成立。第一次迭代的差分成立的概率为 $2^{-43}$。\n使用相同的方法也可以对 $M_1$ 进行修改。修改之后，第二次迭代的差分成立的概率为 $2^{-37}$。\n多重信息修改(Multi-message Modification)。进一步观察发现，通过进行多重信息修改甚至有可能使某些前 $32$ 步循环的充分条件得到满足。\n例如，如果 $a_{5,32}=1$，可以通过修改 $m_1,m_2,m_3,m_4,m_5$ 来将其修正为 $a_{5,32}=0$，以致在 $2$ 到 $6$ 步循环中生成一个局部碰撞，同时保持第一阶段的所有条件仍然成立，见表 1 。其它的条件也可以通过类似的修改方式或其它更加精确的修改方法来进行修正。使用这种修改方法后，表 4 的第 $2$ 到第 $4$ 阶段中还剩 $37$ 个条件未被确定，表6中的第 $2$ 到第 $4$ 阶段中还剩 $30$ 个条件未被确定。因此，第一次迭代差分出现的概率为 $2^{-37}$，第二次迭代差分出现的概率为 $2^{-30}$。\n4.5 The Differential Attack on MD5\n根据以上内容，易得本文对 MD5 的攻击方法。接下来将详细介绍如何找到如下形式的两个信息块产生的碰撞。\n$$\n\\varDelta H_0 \\stackrel{(M_0,M_0’),2^{-37}}{\\longrightarrow}\n\\varDelta H_1 \\stackrel{(M_1,M_1’),2^{-30}}{\\longrightarrow}\n\\varDelta H=0\n$$\n\n\n重复以下步骤，直到找到第一个信息块 $M_0$：\n$\\text{(a)}$ 随机初始化一个信息 $M_0$；\n$\\text{(b)}$ 使用前面所述的信息修改技术对 $M_0$ 进行修改；\n$\\text{©}$ 然后，使用 $M_0$ 和 $M_0’=M_0+ \\varDelta M_0$ 产生第一次迭代的差分\n$$\n\\varDelta M_0 \\longrightarrow (\\varDelta H_1, \\varDelta M_1)\n$$\n其出现概率为 $2^{-37}$；\n$\\text{(d)}$ 将 $M_0$ 和 $M_0’$ 代入压缩函数，检验是否所有差分特征全部成立。\n\n\n重复以下步骤，直到产生一个碰撞：\n$\\text{(a)}$ 随机初始化一个信息 $M_1$；\n$\\text{(b)}$ 使用前面所述的信息修改技术对 $M_1$ 进行修改；\n$\\text{©}$ 然后，使用 $M_1$ 和 $M_1+ \\varDelta M_1$ 产生第二次迭代的差分\n$$\n(\\varDelta H_1, \\varDelta M_1) \\longrightarrow \\varDelta H=0\n$$\n其出现概率为 $2^{-30}$；\n$\\text{(d)}$ 检验这一对信息是否能够导致碰撞。\n\n\n寻找 $(M_0,M_0’)$ 的复杂度不超过运行 $2^{39}$ 次 MD5 运算的时间。得到另一个信息 $M_0’$ 只需要更改之前 $M_0$ 的最后两个字。因此，寻找 $(M_0,M_0’)$ 只需要进行一次对 $M_0$ 的前 $14$ 个字的单一信息修改即可，时间消耗可以忽略不计。对于每一个选定的信息 $M_0$，只需要对最后 $2$ 个字进行 $2$ 次单一信息修改，以及在第二阶段中进行 $7$ 次多重信息修改分别对 $7$ 个条件进行修正，每次多重信息修改只需要很少的操作步骤。因此对于每一个选定的消息，所有两种修改的所用的总时间不超过两次 MD5 运算的时间。\n根据第一次迭代差分的概率易知，该攻击方法中寻找 $(M_0,M_0’)$ 的复杂度不超过 $2^{39}$ 次 MD5 操作的时间。\n表 2 给出了两个 MD5 碰撞的例子。需要注意的是，两个碰撞的第一个 $512$ 位信息块完全相同，并且当给定的第一个信息块满足所有的攻击标准时，很容易就能找到能够产生碰撞的第二个信息块 $(M_1,M_1’)$。\n5 Summary\n本文介绍了一种十分强大的哈希函数攻击方法，并表明使用该方法来寻找MD5碰撞是很容易实现的。该攻击同时也能够有效地破解如 HAVAL-128、MD4、RIPEMD 和 SHA-0 等哈希函数，具体分析结果如下：\n\n寻找 MD4 碰撞的时间复杂度：不使用多重信息修改大约需要 $2^{23}$ 次MD4运算，使用多重信息修改大约需要 $2^8$ 次 MD4 运算。\n寻找 HAVAL-128 碰撞的时间复杂度：不使用多重信息修改大约需要 $2^{13}$ 次HAVAL-128运算，使用多重信息修改大约需要 $2^7$ 次 HAVAL-128 运算。\n寻找 RIPEMD 碰撞的时间复杂度：不使用多重信息修改大约需要 $2^{30}$ 次 RIPEMD 运算，使用多重信息修改大约需要 $2^{18}$ 次 RIPEMD 运算。\n寻找 SHA-0 碰撞的时间复杂度：不使用多重信息修改大约需要 $2^{61}$ 次 SHA-0 运算，使用多重信息修改大约需要 $2^{45}$ 次 SHA-0 运算。\n\n\n\n\n\n\n\n","categories":["Paper Reading"],"tags":["Cryptography","Information Security"]},{"title":"(MLSys 2020) TQT - Trained Quantization Thresholds for Accurate and Efficient Fix-point Inference of Deep Neural Networks","url":"/2021/12/02/tqt/","content":"本文提出了 Training Quantization Thresholds (TQT)：一种基于 QAT 的学习均匀、对称、Per-tensor 量化器的截断阈值的方法。TQT 在训练时使用了 STE ，并将量化步长限制在了 Power-of-Two，采用 PoT 形式的量化步长在推理时仅使用整型加法和移位运算，利于硬件部署。本文对 TQT 的鲁棒性进行了数学分析，并在多个 CNN 上进行了 ImageNet 图像分类任务。实验结果表明：TQT 在 MobileNet 等之前较难量化的神经网络上在少于 5 epochs 的训练就达到了接近浮点的精度。本文相关 Github 项目：Graffitist\n\n\n\n\nTitle: Trained Quantization Thresholds for Accurate and Efficient Fix-point Inference of Deep Neural Networks\nVenue: MLSys 2020\nAuthor(s): Sambhav R. Jain, Albert Gural, Michael Wu, Chris H. Dick\nInstitution(s): Xilinx Inc, Stanford University\nLink(s): arxiv:1903.08066\nProject(s): Graffitist, Vitis-AI\n\n\nIntroduction\n本文主要关注以下三个问题：\n\n之前的量化工作大部分是基于 PTQ 的静态量化，即对预训练模型进行校准得到的截断阈值在之后保持固定不变，相比 QAT 方法得到的量化截断阈值的泛化能力更差。\nPTQ 校准中定义的 Per-Tensor 或 Per-Channel 的 Quantization Error 所用的 Metrics (KLD、MSE等) 都是基于经验设计的。这些 Metrics 与模型最终的 Loss 是否存在相关性，现在业界依然没有理论上的数学证明。相比起那些优化人工给定的 Metrics 的 PTQ 方法，使用 Training-based 的方法更有效，在理论上也更有说服力。\n2018 年 Google 的论文提出的 Integer-Only Inference 量化方案中的比例系数 M 是 dyadic 型的参数而非 PoT 型的参数，会存在一定的高位整型数存储和乘法运算，这会对硬件的存储和运算造成负担。更理想的全整型推理量化方案将量化步长限制在 PoT 的形式，这样可以完全用移位来代替量化步长的乘法运算，更利于硬件加速和实现。\n\nRelated Works\n\nSTE arxiv:1308.3432\nInteger-Arithmetic-Only Inference (CVPR 2018) arxiv:1712.05877\nPACT arxiv:1805.06085\nLSQ (ICLR 2020) arxiv:1902.08153\n\nTrained Quantization Thresholds (TQT)\nQuantizer Constraints\nTQT 的采用的均匀量化公式如下：\n$$\nr=s·(q-z)\n\\tag{2}\n$$\n其中 $s$ 为量化步长，$z$ 为量化零点，$q$ 为量化后的整型数值。为了简化量化运算，TQT采用了对称量化，令 $z=0$ 省略多项式乘法中含量化零点的展开项：\n$$\nr=s·q\n\\tag{3}\n$$\n同时，TQT 要求量化步长 $s$ 限制为 PoT 的形式，即 $s=2^{-f}$，其中 $f$ 为fractional length，可以理解为浮点数小数点的位置，是一个有符号整型数。这样在计算 $r=s·q$ 时就可以用移位运算代替乘法运算。\nForward Pass\nTQT 完整的量化前向计算包含缩放 (Scaling)、取整 (Rounding)、截断 (Saturation)、反量化 (De-quant) 四个计算步骤，量化函数如下：\n$$\nq(x;s)=\\text{clip} \\left( \\left\\lfloor \\frac{x}{s} \\right\\rceil ;n,p \\right)·s\n\\tag{4}\n$$\n其中 $q(x;s)$ 表示量化函数，$x$ 表示输入 Tensor，$s$ 表示量化步长，$\\text{clip}$ 表示区间限定函数，$n$ 和 $p$ 分别为区间上下界。\n量化参数取值根据输入 Tensor 是否有符号决定。\n\n输入为有符号数 (signed)：\n$$\nn=-2^{b-1}, p=2^{b-1}-1, s=\\frac{2^{\\left\\lceil \\text{log}_2\\thinspace t \\right\\rceil}}{2^{b-1}}\n$$\n输入为无符号数 (unsigned)：\n$$\nn=0, p=2^b-1, s=\\frac{2^{\\left\\lceil \\text{log}_2\\thinspace t \\right\\rceil}}{2^b}\n$$\n其中 $t$ 是原始的量化截断阈值，实际上的量化截断阈值的取值是大于 $t$ 的最小的 PoT 型数值 $2^{\\left\\lceil \\text{log}_2\\thinspace t \\right\\rceil}$。\n\nBackward Pass\nTQT 在训练过程中同时优化输入 $x$ 和量化步长 $s$。为了更方便表示反向传播的梯度，可以改写前向传播的量化函数中的 $\\text{clip}$ 函数成为如下的分段函数形式：\n$$\nq(x;s)=\n\\left\\lbrace\n\\begin{aligned}\n&amp; \\left\\lfloor \\frac{x}{s} \\right\\rceil ·s &amp;&amp; \\text{if } n\\leq\\left\\lfloor \\frac{x}{s} \\right\\rceil \\leq p, \\\\\n&amp; n·s &amp;&amp; \\text{if } \\left\\lfloor \\frac{x}{s} \\right\\rceil &lt; n, \\\\\n&amp; p·s &amp;&amp; \\text{if } \\left\\lfloor \\frac{x}{s} \\right\\rceil &gt; p. \\\\\n\\end{aligned}\n\\right.\n\\tag{5}\n$$\n在进行反向传播时，TQT 对不可反向传播的取整运算采用了 STE，即对于 $\\left\\lceil x \\right\\rceil \\neq x, \\left\\lfloor x \\right\\rfloor \\neq x,  \\left\\lfloor x \\right\\rceil \\neq x$ 的情况时，令$\\frac{\\partial \\left\\lceil x \\right\\rceil}{\\partial x} = \\frac{\\partial \\left\\lfloor x \\right\\rfloor}{\\partial x} = \\frac{\\partial \\left\\lfloor x \\right\\rceil}{\\partial x} = 1$。\n这样得到的量化函数 $q$ 关于 $x$ 和 $s$ 的梯度如下：\n$$\n\\nabla_s q(x;s)=\n\\left\\lbrace\n\\begin{aligned}\n&amp; \\left\\lfloor \\frac{x}{s} \\right\\rceil - \\frac{x}{s} &amp;&amp; \\text{if } n\\leq\\left\\lfloor \\frac{x}{s} \\right\\rceil \\leq p, \\\\\n&amp; n &amp;&amp; \\text{if } \\left\\lfloor \\frac{x}{s} \\right\\rceil &lt; n, \\\\\n&amp; p &amp;&amp; \\text{if } \\left\\lfloor \\frac{x}{s} \\right\\rceil &gt; p. \\\\\n\\end{aligned}\n\\right.\n\\tag{6}\n$$\n注意到 $\\nabla_{\\text{log}_2 \\thinspace t} s=s \\text{ ln}(2)$，上式可继续转化为：\n$$\n\\nabla _{\\text{log}_2\\thinspace t} q(x;s)=s \\text{ ln}(2)·\n\\left\\lbrace\n\\begin{aligned}\n&amp; \\left\\lfloor \\frac{x}{s} \\right\\rceil - \\frac{x}{s} &amp;&amp; \\text{if } n\\leq\\left\\lfloor \\frac{x}{s} \\right\\rceil \\leq p, \\\\\n&amp; n &amp;&amp; \\text{if } \\left\\lfloor \\frac{x}{s} \\right\\rceil &lt; n, \\\\\n&amp; p &amp;&amp; \\text{if } \\left\\lfloor \\frac{x}{s} \\right\\rceil &gt; p. \\\\\n\\end{aligned}\n\\right.\n\\tag{7}\n$$\n同理，输入 $x$ 的梯度如下：\n$$\n\\nabla _x q(x;s)=\n\\left\\lbrace\n\\begin{aligned}\n&amp; 1 &amp;&amp; \\text{if } n \\leq \\left\\lfloor \\frac{x}{s} \\right\\rceil \\leq p, \\\\\n&amp; 0 &amp;&amp; \\text{otherwise} \\\\\n\\end{aligned}\n\\right.\n\\tag{8}\n$$\nInterpretation of Gradients\n上面给出了量化器 $q$ 对 $x$ 和 $\\text{log}_2 \\thinspace t$ 的梯度公式，为了直观地理解量化器在 QAT 反向传播时梯度的变化情况，不妨考虑下面这样一个简单的场景：\n使用最小平方误差 (least-square error) L2损失函数 $L = (q(x;s) - x)^2/2$ 优化一个 TQT 量化器，$L$ 对 $x$ 和 $\\text{log}_2 \\thinspace t$ 的梯度如下：\n$$\n\\begin{align}\n\\nabla _{\\text{log}_2 \\thinspace t} L &amp;= (q(x;s) - x) · \\nabla _{\\text{log}_2 \\thinspace t} q(x;s) \\tag{9}\\\\\n\\nabla _x L &amp;= (q(x;s) - x) · (\\nabla _x q(x;s) - 1) \\tag{10}\\\\\n\\end{align}\n$$\n\n\nFigure 1\n\nFigure 1 展示了在 $b=3, t=1.0$ 时有符号和无符号情况下函数值和各参数梯度的变化曲线。第一行为量化函数，第二行为损失函数。\n考虑到取整 $\\left\\lfloor · \\right\\rceil$ 函数和截断 $\\text{clip}$ 函数的共同作用，实际上输入 $x$ 的真实范围为 $[s·(n-0.5), s·(p+0.5)]$。从Figure 1可以看出：\n\n当输入 $x$ 在真实范围内时，$L$ 对 $\\text{log}_2 \\thinspace t$ 的梯度非负，对 $x$ 的梯度为 $0$；\n当输入 $x$ 在真实范围外时，$L$ 对 $\\text{log}_2 \\thinspace t$ 的梯度恒负，对 $x$ 的梯度左负右正。\n\n\n    \n    Figure 2\n\nFigure 2 展示了通常情况下呈钟型分布(例如高斯分布)的输入 $x$ 的 $\\text{log}_2 \\thinspace t$ 梯度的变化情况：\n\n左图：绝大部分输入值 $x$ 都在真实范围内，对 $\\text{log}_2 \\thinspace t$ 的累计梯度为正，梯度下降更新使得 $\\text{log}_2 \\thinspace t$ 减小向分布中心靠近；\n中间图：对于在真实范围外的输入，其对 $\\text{log}_2 \\thinspace t$ 的累计梯度为负，梯度下降使 $\\text{log}_2 \\thinspace t$ 向外扩大。\n右图：训练收敛时，正负梯度相加为零，此时的截断阈值满足 PoT 型且能够使模型精度损失尽可能低。\n\nTQT 在 QAT 过程中同时优化输入 $x$。与对截断阈值梯度的分析类似：\n\n当输入 $x$ 在真实范围内时，$L$ 对 $x$ 的梯度为 $0$，即截断范围内输入保持不变；\n当输入 $x$ 在真实范围外时，$L$ 对 $x$ 的梯度左负右正。\n这表明，对于范围外的输入 $x$，梯度下降更新使得输入 $x$ 向范围内变化，使 $x$ 越来越趋向分布中心，减少分布中的离群点。\n\nComparison to Clipped Threshold Gradients\n\n与 TensorFlow 的 FakeQuant 的比较\n\n\n    \n    Figure 3\n\nTensorFlow 框架的伪量化模块 FakeQuant 参考的量化函数如下：\n$$\nq(x;n,p)=\\left\\lfloor \\frac{\\text{clip}(x;n,p)-n}{\\frac{p-n}{2^b-1}} \\right\\rceil · \\frac{p-n}{2^b-1} + n\n\\tag{11}\n$$\nFigure 3 展示了在 $b=3, n=-1.125, p=0.875$ 时 TensorFlow 的 FakeQuant 模块的函数值和各参数梯度的变化曲线。与TQT的量化方法相比，FakeQuant学习截断阈值 $n$ 和 $p$，在反向传播阶段将取整函数简化为恒等函数。由Figure 3可以看出， $n$ 和 $p$ 的梯度恒为正，因此在梯度下降更新时只能向外侧变化，这样的学习方法得到的结果更贴近于PTQ的min/max截断阈值取值，而不是在量化模型精度和截断范围之间进行trade-off。\n\n与 PACT 的比较\n\nPACT (PArameterized Clipping acTivation) 为了能够将量化截断阈值引入 QAT 训练过程中进行优化，提出了 Clipped ReLU 激活函数：\n$$\ny=PACT(x)=0.5(|x|-|x-\\alpha|+\\alpha)=\n\\left\\lbrace\n\\begin{aligned}\n&amp; 0, &amp;&amp; x \\in (-\\infty, 0) \\\\\n&amp; x, &amp;&amp; x \\in [0, \\alpha) \\\\\n&amp; \\alpha, &amp;&amp; x \\in [\\alpha, +\\infty)  \\\\\n\\end{aligned}\n\\right.\n$$\n输出激活值对截断阈值 $\\alpha$ 的梯度使用 STE 近似如下：\n$$\n\\frac{\\partial y_q}{\\partial \\alpha} =\n\\left\\lbrace\n\\begin{aligned}\n&amp; 0, &amp;&amp; x \\in (-\\infty, \\alpha) \\\\\n&amp; 1, &amp;&amp; x \\in [\\alpha, +\\infty) \\\\\n\\end{aligned}\n\\right.\n\\tag{1}\n$$\n在 PACT 方法中，$\\alpha$ 的梯度只与 $x$ 取值在 $\\alpha$ 的哪一侧有关，显然只有在 $\\alpha$ 右侧的值能够造成影响，因此在训练过程中 $\\alpha$ 会趋向于 $x$ 的右端取值的最大值。为了避免这一现象，PACT 在训练时在损失函数中加入了对 $\\alpha$ 的 L2 正则项，但是 L2 正则项的超参数 $\\lambda_{\\alpha}$ 只能人工设定，不能自动地在 QAT 过程中进行优化，因此相比 TQT 而言略显不足。有关 PACT 的其它具体细节请查阅相关文献和阅读笔记。\nFramework of TQT\n本文相关的 Github 项目 Graffitist 是基于 TensorFlow 框架和 TQT 方法实现的端到端式的量化解决方案。Graffitist 支持对多种神经网络进行计算图优化和模型量化。\nGraph Optimizations\nGraffitist 实现了多种对计算图的优化方式。包括 Fuse-BN、多重 concat 拆分成单一 concat、将平均池化层转化为深度卷积层等等。\nQuantization Modes\nGraffitist 实现了多种量化模式，包括 PTQ 的静态量化模式和 QAT 的量化模式。\nLayer Precisions\nGraffitist 分别实现了针对计算层、激活层、平均池化层和 concat 操作的量化计算位数设计。\nFused Kernel Implementation\nGraffitist 中打包了为 CPU/GPU 预编译的融合后的量化内核。经过融合后的量化器占用内存低，从而可以使用更大的 batch size 来实现加速。\nExperiments\n此处只展示实验结果。省略了实验的初始化和实现细节，具体细节请参看原论文。\nResults\n\n    \n    Table 3\n\nTable 3 展示了对 12 种不同的神经网络模型在 ImageNet 分类任务上的验证准确率。wt 表示只训练模型权重参数，wt,th 表示模型权重和截断阈值同时训练。所有再训练过程均不超过5 epochs。\nDiscussion\nInsights from TQT\n本文观察到了关于 TQT 的一些值得注意的实验现象：\n\nQAT 方法得到的模型预测精度比 PTQ 静态量化的更高，这也是期望之中的结果。\n对于 VGG、Inception、ResNet 等 INT8 量化难度较低的模型，固定截断阈值只对模型参数进行再训练的效果已经足够好了，此时也采用 TQT 方法也没有更多精度上的提升。\n对于 MobileNet、DarkNet 等 INT8 量化难度较高的模型，同时训练模型参数与截断阈值要比只训练模型参数的精度高 $4 \\%$，甚至接近 FP32 模型的精度。\n对于更低比特的 INT4 量化，只训练模型参数已经很难提升精度了，因此 INT4 量化必须使用 TQT 方法才能有效维持模型精度。\n\nMobileNet Comparisons\n\n    \n    Table 1\n\n对于较难量化的 MobileNet，先前的大量研究已经表明：采用简单的对称、per-tensor 的 PTQ 方法去量化 MobileNet 通常是 detrimental 的。其中一个主要的原因是深度可分离卷积层的权重数值分布是不规则的且不同通道间的数值范围差异很大。然而，使用 TQT (wt,th) 方法对 MobileNet 进行 QAT 的结果已经达到了 FP32 的精度。这里简单地与 Google 的 QAT 结果进行对比，对比结果如 Table 1 所示。尽管 TQT 对量化步长的约束比 Google 的方法更为严格，但是量化后精度却更高，这也体现了 TQT 方法的优越性。\n\n    \n    Figure 5\n\nFigure 5 中展示了使用 TQT 方法进行 QAT 再训练的 MobileNet v1 前后模型权重参数分布情况。从图中可以体现出量化截断阈值在截断范围和模型精度的 trade-off 的重要性。\n\n    \n    Figure 6\n\nFigure 6 中展示了 INT8 和 INT4 量化不同模型所有层的 fractional length 的始末差异分布情况。可以发现 INT8 量化比起 INT4 量化 fractional length 的变化程度更大，这也与 TQT 方法本身的特点有关系：在能够表示的比特位较多时，该方法会更偏向于扩大截断范围；相反，当比特位较少时，截断范围的扩大和模型精度的 trade-off 更加明显。\nConclusion\n本文提出了一种通用的学习量化截断阈值的方法 TQT。其特性如下：\n\nTQT 方法能够将量化步长限制在 PoT 型数值，适用于大多数支持定点运算的硬件。\nTQT 学习量化截断阈值的过程能够体现出截断范围和模型精度的 trade-off，并在 INT8 和 INT4 量化中取得很高的精度。\nTQT 方法具有鲁棒性和快速收敛性等特点。\n\n此外，本文就 TQT 方法基于 TensorFlow 上建立了 Graffitist 项目，并基于 ImageNet 图像分类任务对多个经典模型进行量化实验，最后，本文就某些值得关注的实验现象进行阐述。\nAppendices\n本文的附录内容简单总结如下：\nA Cost of Affine Quantizer\n本节主要对量化器的计算开销进行分析讨论。\n附录 A.1 对量化乘法运算进行详细分析，表明忽略量化零点 $Z$ 后可以有效地降低量化计算复杂度。\n附录 A.2 分析了浮点型、dyadic 型和 PoT 型量化步长的乘法运算，其中dyadic型详见Google论文，PoT型可以直接使用移位运算代替乘法运算。\nB Log Threshold Training\n本节主要针对QAT过程进行分析，主要从数值稳定性、尺度不变性和收敛性三个方面进行讨论分析。\n附录B.1分析训练的数值稳定性(Numeral Stability)。比起训练范围受限的原始截断阈值 $t \\in \\mathbb{R}^+$，不如直接训练 $\\text{log} _2 \\thinspace t \\in \\mathbb{R}$，并且 $\\text{log}$ 函数的形式与PoT型正好吻合。\n附录B.2分析训练的尺度不变性(Scale Invariance)，也就是说其梯度应该尽量与其取值大小无关，而只与其它因素有关。\n附录B.3分析训练的收敛性(Convergence)，并给出了不同比特位下Adam优化器的参数取值参考。\nC Adam Convergence\n本节主要针对TQT在Adam优化器上的收敛性进行详细分析。\nD Best or Mean Validation\n本节主要针对该工作的实验结果筛选进行解释。本文在MobileNetV1和VGG16上的1000次验证平均准确率与最高准确率的差距分别为 $0.1\\%$ 和 $0.2\\%$，表明结果的真实性和可靠性。\n","categories":["Paper Reading"],"tags":["Deep Learning","Quantization","MLSys 2020","QAT","STE","Integer Only","Symmetric","Uniform"]},{"title":"Hessian-based Post-Training Quantization","url":"/2022/03/15/hessian_ptq/","content":"2022-03 人工智能前沿技术研讨课 个人课程作业留档\n\n引言\n问题背景\n2012 年，AlexNet 在 ILSVRC 比赛上大放异彩，人工智能也随之走入了以深度学习技术为主的新的发展时期。在之后的几年里，基于深度学习的算法或模型在任务上的性能改进都与模型的参数量成正相关。复杂的模型的计算时间和空间复杂度都很高，在资源受限的硬件上部署仍然具有很大挑战。\n\n神经网络量化的基本概念\n量化\n量化（Quantization）这一概念源自通信和信息学领域，是指一种从大（通常是连续）范围的输入值映射到（通常是离散）范围的输出值的方法。在数值计算问题中，量化问题指一组连续的实数值应该如何映射到一组取值固定的离散数值集合上，从而能够最大限度地减少数值表示所需的位数并最大限度地提高计算的精度。近年来，由于基于深度学习的神经网络模型的复杂度不断提高，对神经网络的量化问题的研究逐渐成为人工智能技术应用和神经网络计算优化的重要子领域。\n针对深度神经网络的量化一般有两个目的：网络压缩和推理加速。网络压缩指通过降低模型参数（包括权重和激活值）的数值表示所需的位数，达到降低内存容量和带宽的效果，例如将一个参数全部是 32 位浮点数的神经网络的权重和激活值全部量化到 8 位整型数值，其内存占用和带宽理论上均可减少至原来的四分之一。而实现推理加速则是利用量化从高精度映射到低精度数值表示的特性，进行进一步的低精度（如整型）的计算优化，减少模型计算推理的时间，例如对 8 位整型数的乘法运算进行优化，可以理论上实现相比 32 位浮点数的乘法运算的速度快 16 倍的加速效果。\n量化时的浮点数离散化操作会造成精度损失，因此在量化操作之后得到的值会与原真实值存在差异，这个差异叫作量化误差（Quantization Error）。直观上来说，将全精度的数值映射为低精度的数值会不可避免地导致信息丢失，这个过程中会对神经网络模型引入量化误差，导致模型预测精度的下降。但是实际上许多量化神经网络的表现与原始神经网络相差无几。关于量化误差与模型预测精度的相关性，目前尚未出现严谨的理论证明。一个可能的解释是：当前多数神经网络模型都存在严重的参数冗余，模型参数的自由度较高，因此对于参数的量化具有较高的鲁棒性。\n\n量化的数学表示\n量化可以表示为一个单射函数 $Q$，该函数将输入的浮点数值集合 $X$ 映射到离散的数值集合 $Y$。函数 $Q$ 在过去的研究中有多种实现方式，其中使用得最为广泛的是对称仿射量化（Symmetric Affine Quantization），或者叫作对称线性量化（Symmetric Linear Quantization）。对称线性量化的一般形式如下：\n$$\n\\begin{align*}\n&amp;Q(\\mathbf{x}) = \\text{clip}\\left(\\text{round}\\left(\\frac{\\mathbf{x}}{s}\\right)\\right)\\\\\n&amp;\\hat{\\mathbf{x}} = s \\cdot Q(\\mathbf{x})\n\\end{align*}\n$$\n对于某一粒度规模（Layer-wise 或者 Channel-wise）的参数，对称线性量化使用一个缩放系数 $s$ 来调整原始集合的数值范围到固定的数值范围，然后经过舍入 $\\text{round}$ 和截断 $\\text{clip}$ 的方式实现取值的离散化，最后重新使用缩放系数 $s$ 来恢复到原始集合的数值范围。使用对称线性量化的好处是：在求解量化问题时需要优化的参数只有量化的缩放系数 $s$，并且量化神经网络在实际进行乘法运算时，可以提取出计算过程中不同的缩放系数 $s$，先进行低精度的整数乘法，然后统一进行缩放运算，这样可以简化运算，实现计算加速效果，如 Figure. 3 所示。\n\n后训练量化和量化感知训练\n后训练量化（Post-Training Quantization, PTQ），又称离线量化，指的是将一个预训练的神经网络模型通过参数优化等方法获得一个量化的神经网络模型，一般不会涉及网络的再训练过程。离线量化方法可以快速获取一个量化模型，可以节省训练所需的计算资源，并且更适合快速迭代的应用场景。针对激活值的离线量化需要使用一个小规模的数据集作为校准集（Calibration Dataset），通过对校准集进行推理来统计激活值的数值范围，从而实现对激活的量化优化。\n量化感知训练（Quantization-Aware Training，QAT），又称在线量化。与离线量化不同，在线量化将量化映射函数事先插入到模型的计算图中需要被量化的参数的位置，并对模型使用完整的训练集进行训练。再训练过程不仅会优化原始任务的损失，同时也会优化量化误差带来的量化损失。使用量化感知训练得到的量化模型通常比后训练量化得到的模型的预测准确度更高，但需要消耗大量的计算资源，并且超参数的设定也会对量化模型的收敛产生影响。\n\nHessian 矩阵\n海森矩阵（Hessian Matrix），或称作黑塞矩阵、海瑟矩阵等，是由多元函数的二阶偏导数构成的方阵。对于复杂的优化问题，其目标函数往往较为复杂。为了使问题简化，可以将目标函数在某点的邻域进行二阶泰勒展开（Taylor Expansion）来逼近原函数，其中的二阶项就包含 Hessian 矩阵。\n例如，对于一个多元函数 $\\mathcal{F}(\\mathbf{x})$，其在点 $\\mathbf{x}_0$ 处的二阶泰勒展开可以表示为：\n$$\n\\begin{align*}\n\\mathcal{F}(\\mathbf{x}) \\approx \\mathcal{F}(\\mathbf{x}_0) + \\nabla \\mathcal{F}(\\mathbf{x}_0)^{T} \\cdot \\Delta \\mathbf{x} + \\frac{1}{2} \\Delta \\mathbf{x}^{T} \\cdot \\nabla^{2} \\mathcal{F}(\\mathbf{x}_0) \\cdot \\Delta \\mathbf{x},\n\\end{align*}\n$$\n其中的二阶偏导数矩阵 $\\mathbf{H}^{(\\mathbf{x})} = \\nabla^{2} \\mathcal{F}(\\mathbf{x}_0)$ 就是 $\\mathcal{F}$ 在点 $\\mathbf{x}_0$ 处的 Hessian 矩阵，$\\Delta \\mathbf{x} = \\mathbf{x} - \\mathbf{x}_0$ 是 $\\mathbf{x}$ 相对于 $\\mathbf{x}_0$ 的偏移量。\n在深度神经网络的优化问题中，Hessian 矩阵可以用来描述损失函数在参数空间中的 Loss landscape。已有的工作例如 PyHessian[1] 通过近似计算 Hessian 矩阵的特征向量（Eigenvalue）、迹（Trace），可以获得关于损失函数的二阶信息，从而更好地对不同模型和优化器的性能进行比较和分析。\n本次介绍的三篇论文 AdaRound[2]、BRECQ[3]、QDrop[4] 均使用了 Hessian 矩阵来优化量化神经网络模型的性能。\nAdaRound\n\n\nTitle: Up or Down? Adaptive Rounding for Post Training Quantization\nVenue: ICML 2020\nAuthor(s): Markus Nagel, Rana Ali Amjad, Mart van Baalen, Christos Louizos, Tijmen Blankevoort\nInstitution(s): Qualcomm AI Research\nLink: arxiv:2006.10518\n\n\n前言\n在进行神经网络的后训练量化时，预训练模型的权重参数 $\\mathbf{w}$ 和对应的量化缩放系数 $\\mathbf{s}$ 已经确定，则对于量化误差 $\\Delta \\mathbf{w}$ 也已确定。假设预训练模型在原始任务上的全局损失为 $\\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})$，其中 $\\mathbf{x}$ 为输入样本，$\\mathbf{y}$ 为预测真值，$\\mathbf{w}$ 为模型参数。将对模型参数 $\\mathbf{w}$ 的量化看作为对 $\\mathbf{w}$ 的一个扰动，即 $Q(\\mathbf{w})=\\mathbf{w}+\\Delta \\mathbf{w}$。那么，该预训练模型的量化对全局损失的期望及其二阶泰勒展开的近似可以表示为：\n$$\n\\begin{align*}\n&amp; \\mathbb{E}[\\mathcal{L}(\\mathbf{x}, \\mathbf{y}, Q(\\mathbf{w}))]\\\\\n\\overset{(a)}{\\approx} &amp; \\mathbb{E}[\\Delta\\mathbf{w}^{T} \\cdot \\nabla \\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w}) + \\frac{1}{2} \\Delta\\mathbf{w}^{T} \\cdot \\nabla^{2} \\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w}) \\cdot \\Delta\\mathbf{w}]\\\\\n= &amp; \\Delta \\mathbf{w}^{T} \\cdot \\mathbf{g}^{(\\mathbf{w})} + \\frac{1}{2} \\Delta \\mathbf{w}^{T} \\cdot \\mathbf{H}^{(\\mathbf{w})} \\cdot \\Delta \\mathbf{w},\n\\end{align*}\n$$\n其中 $\\mathbf{g}^{(\\mathbf{w})}$ 为原始任务损失 $\\mathcal{L}$ 对 $\\mathbf{w}$ 的一阶偏导数向量，$\\mathbf{H}^{(\\mathbf{w})}$ 为 Hessian 矩阵：\n$$\n\\begin{align*}\n\\mathbf{g}^{(\\mathbf{w})} &amp; = \\mathbb{E} \\left[ \\nabla_{\\mathbf{w}} \\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w}) \\right]\\\\\n\\mathbf{H}^{(\\mathbf{w})} &amp; = \\mathbb{E} \\left[ \\nabla_{\\mathbf{w}}^{2} \\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})\\right].\n\\end{align*}\n$$\n由于使用的是已训练至收敛的预训练模型，可以假设原任务损失对模型参数 $\\mathbf{w}$ 的一阶偏导数为 $0$，即泰勒展开一阶项为 $0$。常数项也相互抵消。因此，量化对全局损失的期望可以近似用泰勒展开的二阶项来表示：\n$$\n\\mathbb{E}[\\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w} + \\Delta \\mathbf{w}) - \\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})] = \\frac{1}{2} \\Delta \\mathbf{w}^{T} \\cdot \\mathbf{H}^{(\\mathbf{w})} \\cdot \\Delta \\mathbf{w}.\n$$\n这样，针对网络模型参数的量化问题就转化为了对包含 Hessian 矩阵的二次型的优化问题。为了更好地理解这个问题，不妨设 $\\mathbf{w}$ 包含两个参数 $\\mathbf{w}^{T}=[w_{1}, w_{2}]$，对应的量化误差为 $\\Delta \\mathbf{w}^{T}=[\\Delta w_{1}, \\Delta w_{2}]$，若对应的 Hessian 矩阵为：\n$$\n\\mathbf{H}^{(\\mathbf{w})} =\n\\begin{bmatrix}\n1 &amp; 0.5 \\\\\n0.5 &amp; 1\n\\end{bmatrix},\n$$\n则量化误差对全局损失的影响可以表示为：\n$$\n\\Delta \\mathbf{w}^{T} \\cdot \\mathbf{H}^{(\\mathbf{w})} \\cdot \\Delta \\mathbf{w} = \\Delta w_{1}^{2} + \\Delta w_{2}^{2} + \\Delta w_{1} \\Delta w_{2},\n$$\n其中位于 Hessian 矩阵对角线上的二次型项恒为非负，其必然会导致量化误差对全局损失的增加。而 Hessian 矩阵的非对角线项可正可负，会导致量化误差之间的交互作用。若扰动方向相反，则可以使得模型全局损失减小。\n\nAdaRound 提出了一个全新的模型量化算法的优化视角：传统的就近舍入（Round to Nearest）方法会将量化误差 $\\Delta \\mathbf{w}$ 的每个分量 $\\Delta w_{i}$ 都舍入到最接近的整数值 $Q(\\Delta w_{i})$，这并不一定是最优解。如 Figure. 5 所示，通过优化量化误差 $\\Delta \\mathbf{w}$ 的每个分量的舍入方向（Up or Down），可能会使得量化误差对全局损失的影响更小。\n方法\n从全局损失到局部损失\nAdaRound 将量化误差对全局损失的影响的期望作为优化目标函数：\n$$\n\\begin{align*}\n\\underset{\\Delta \\mathbf{w}}{\\arg\\min} \\space \\mathbb{E}[\\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w} + \\Delta \\mathbf{w}) - \\mathcal{L}(\\mathbf{x}, \\mathbf{y}, \\mathbf{w})]\n\\end{align*}\n$$\n该目标函数可以转化为包含 Hessian 矩阵的二次型形式：\n$$\n\\begin{align*}\n\\underset{\\Delta \\mathbf{w} ^{(\\ell)}}{\\arg\\min} \\space\n\\mathbb{E}[\\Delta {\\mathbf{w} ^{(\\ell)}} ^{T} \\cdot \\mathbf{H} ^{(\\mathbf{w})} \\cdot \\Delta \\mathbf{w} ^{(\\ell)}]\n\\end{align*}\n$$\n对于常见粒度的参数量化而言，其 Hessian 矩阵的规模很大，很难在有限时间内进行计算和求解。AdaRound 对该问题进行了简化。首先，AdaRound 将参数粒度限制在了 Layer-wise 层级，即每次优化只考虑一个参数层的所有参数。对于模型的每一层 $\\ell$ 中的权重参数 $\\mathbf{W}^{(\\ell)}$，全局损失 $\\mathcal{L}$ 对其的二阶偏导数可以表示为：\n$$\n\\begin{align*}\n\\frac{\\partial^{2} \\mathcal{L}}{\\partial \\mathbf{W} _{i,j} ^{(\\ell)} \\partial \\mathbf{W} _{m,o} ^{(\\ell)}} &amp; = \\frac{\\partial}{\\partial \\mathbf{W} _{m,o} ^{(\\ell)}} \\left[ \\frac{\\partial\\mathcal{L}}{\\partial \\mathbf{z} _{i} ^{(\\ell)}} \\cdot \\mathbf{x} _{j} ^{(\\ell-1)} \\right] \\\\\n&amp; = \\frac{\\partial ^{2} \\mathcal{L}}{\\partial \\mathbf{z} _{i} ^{(\\ell)} \\partial \\mathbf{z} _{m} ^{(\\ell)}} \\cdot \\mathbf{x} _{j} ^{(\\ell-1)} \\cdot \\mathbf{x} _{o}^{(\\ell-1)},\n\\end{align*}\n$$\n那么，损失函数对该层参数的 Hessian 矩阵可以表示为：\n$$\n\\mathbf{H} ^{(\\mathbf{W} ^{(\\ell)})} = \\mathbb{E} \\left[ \\mathbf{x} ^{(\\ell-1)} \\cdot {\\mathbf{x} ^{(\\ell-1)}} ^{T} \\otimes \\nabla _{\\mathbf{z} ^{(\\ell)}} ^{2} \\mathcal{L} \\right],\n$$\n其中 $\\mathbf{x}^{(\\ell-1)}$ 为上一层的输出激活值，$\\mathbf{z}^{(\\ell)}$ 为该层的预激活值，$\\otimes$ 表示 Kronecker 积。AdaRound 分别基于两个不同的假设对该 Hessian 矩阵进行了简化。\n假设一：Hessian 矩阵 $\\mathbf{H} ^{(\\mathbf{W} ^{(\\ell)})}$ 为对角矩阵，即本层网络的神经元输出相互独立。在此假设下，Hessian 矩阵可以简化为：\n$$\n\\mathbf{H} ^{(\\mathbf{W} ^{(\\ell)})} = \\mathbb{E} \\left[\\mathbf{x} ^{(\\ell-1)}\\cdot{\\mathbf{x} ^{(\\ell-1)}} ^{T}\\otimes\\text{diag}(\\nabla _{\\mathbf{z} ^{(\\ell)}} ^{2}\\mathcal{L} _{i,i}) \\right].\n$$\n假设二：损失函数 $\\mathcal{L}$ 对 $\\mathbf{z} ^{(\\ell)}$ 的二阶偏导数是常数，即与输入激活值 $\\mathbf{x} ^{(\\ell - 1)}$ 无关。因此 $\\nabla _{\\mathbf{z} ^{(\\ell)}} ^{2} \\mathcal{L}$ 是一个常数矩阵。这样目标函数就省略了加权项，最终简化为对当前层预激活输出的均方误差（Mean Squared Error, MSE）的形式：\n$$\n\\begin{align*}\n&amp; \\underset{\\Delta \\mathbf{W} _{k,:} ^{(\\ell)}}{\\arg\\min} &amp;&amp; \\mathbb{E} \\left[ \\nabla _{\\mathbf{z} ^{(\\ell)}} ^{2} \\mathcal{L} _{k,k} \\cdot \\Delta \\mathbf{W} _{k,:} ^{(\\ell)} \\mathbf{x} ^{(\\ell-1)} {\\mathbf{x} ^{(\\ell-1)}} ^{T} {\\Delta \\mathbf{W} _{k,:} ^{(\\ell)}} ^{T} \\right]\\\\\n=\\space &amp; \\underset{\\Delta \\mathbf{W} _{k,:} ^{(\\ell)}}{\\arg\\min} &amp;&amp; \\Delta \\mathbf{W} _{k,:} ^{(\\ell)} \\mathbb{E} \\left[ \\mathbf{x} ^{(\\ell-1)}{\\mathbf{x} ^{(\\ell-1)}} ^{T} \\right] {\\Delta \\mathbf{W} _{k,:} ^{(\\ell)}} ^{T} \\\\\n=\\space &amp; \\underset{\\Delta \\mathbf{W} _{k,:} ^{(\\ell)}}{\\arg\\min} &amp;&amp; \\mathbb{E} \\left[ \\left( \\Delta \\mathbf{W} _{k,:} ^{(\\ell)} \\mathbf{x} ^{(\\ell-1)} \\right) ^{2} \\right]\n\\end{align*}\n$$\n通过以上对问题的简化，原始的全局损失转化为了每一层预激活输出 MSE 的局部损失。这样只需要进行逐层进行优化局部损失，即可完成对量化误差的优化。\nAdaRound\n为了对舍入方向进行优化，AdaRound 设计了一种对上述局部损失的优化方法。AdaRound 的目标函数为当前层预激活 MSE 加入了一个正则化项。正则化项的参数 $\\mathbf{V}$ 是与参数 $\\mathbf{W}$ 同维度的可学习参数，用来控制量化误差的舍入方向。AdaRound 的目标函数可以表示为：\n$$\n\\underset{\\Delta \\mathbf{W}^{(\\ell)}}{\\arg\\min} \\space | \\mathbf{W}\\mathbf{x} - \\widetilde{\\mathbf{W}}\\mathbf{x} | _{F} ^{2} + \\lambda f _{reg}(\\mathbf{V}),\n$$\n其中 $\\widetilde{\\mathbf{W}}$ 是经过量化的权重参数：\n$$\n\\widetilde{\\mathbf{W}} = s \\cdot \\text{clip} \\left( \\left\\lfloor \\frac{\\mathbf{W}}{s} \\right\\rfloor + h(\\mathbf{V}),n,p \\right).\n$$\n与常规的对称线性量化函数不同，AdaRound 的量化函数将就近舍入改为向下取整后加上一个整流函数 $h$：\n$$\nh(\\mathbf{V} _{i,j}) = \\text{clip} \\left( \\sigma(\\mathbf{V} _{i,j})(\\zeta - \\gamma) + \\gamma, 0, 1 \\right),\n$$\n整流函数 $h$ 的输入为 $\\mathbf{V}$，输出为普通 Sigmoid 函数 $\\sigma$ 经过范围缩放后再进行截断至范围 $[0,1]$ 之间的结果，表示当前参数的自适应舍入方向。这样设计的目的是为了避免 Sigmoid 函数在趋近 $0$ 或 $1$ 时的梯度消失问题。其中，缩放系数 $\\zeta$ 和 $\\gamma$ 分别设定为 $\\zeta=1.1$ 和 $\\gamma=-0.1$。\n$$\nf _{reg} (\\mathbf{V}) = \\sum _{i,j} 1 - | 2h(\\mathbf{V} _{i,j}) - 1 | ^{\\beta},\n$$\nAdaRound 目标函数的正则项 $f _{reg}$ 使用退火温度参数 $\\beta$ 来指导可学习参数 $\\mathbf{V}$ 的收敛。温度参数 $\\beta$ 的值初始化为较大的数值，在初始阶段加大惩罚力度帮助 $\\mathbf{V}$ 收敛。之后在优化过程中逐渐减小，进一步优化预激活 MSE。\n考虑到神经网络中量化误差会逐层累加，上述优化过程也是逐层进行。为了避免累加的量化误差对最终模型精度的影响，在逐层优化的过程中，当前被量化层的输入是之前所有层经过量化后得到的输出。除此之外，激活函数对量化的影响也需要被考虑到优化范围内。这样就得到了 AdaRound 最终的优化目标函数。\n$$\n\\underset{\\mathbf{V}}{\\arg\\min} \\space \\left| f _{a}(\\mathbf{W} \\mathbf{x}) - f _{a}(\\widetilde{\\mathbf{W}} \\hat{\\mathbf{x}})  \\right| _{F} ^{2} + \\lambda f _{reg} (\\mathbf{V}),\n$$\n实验\n消融实验\n本文从以下五个方面展开了消融实验，验证本文所提出方法的有效性。\n从全局损失到局部损失目标函数的选择。本文对比了原始的就近舍入量化、Hessian 矩阵二次型、预激活 MSE 以及 AdaRound 目标函数的模型参数重建量化效果，结果表明，使用 AdaRound 的目标函数进行优化的量化效果最优。\n\nAdaRound 整流函数 $h$ 的选择。本文对比了使用退火参数的 Sigmoid 函数、普通 Sigmoid 函数加正则项函数以及整流函数 $h$ 加正则化项三种情况，结果表明，使用整流函数 $h$ 加正则化项能够在网络整体的量化上达到更好的效果。\n\n使用 STE 进行优化。本文对比了使用辅助变量进行优化重建的方法以及使用直通估计器（Straight Through Estimator，STE）进行量化感知训练的方法。结果表明，使用重建的方法要比使用 STE 的 QAT 方法效果要更好一些，原因是 STE 可能会导致量化前后的参数反向传播时梯度不匹配（Gradient Mismatch）问题。\n\n优化参数缩放系数的指标。AdaRound 仍然沿用原始的对称线性量化函数，因此在进行重建前需要提前优化确定参数的缩放系数 $s$，一般采用网格搜索的方法来穷举某一目标函数的值，取某种指标的最优值点来确定缩放系数。本文对比了最大最小值差、权重参数 MSE 和激活 MSE 三种指标函数，结果表明，使用权重 MSE 或激活 MSE 对 AdaRound 初始化都能获得较优的量化效果。\n\n校准集样本量大小。本文对校准集的大小对量化的影响进行了探究。结果表明，校准集样本数量越大，量化导致的模型预测损失越低。另外，AdaRound 对校准集的采样来源具有较高的鲁棒性，使用不同任务的数据集作为校准集只会有很微小的精度下降。\n\n对比实验\nAdaRound 主要对比了 2020 年以前在 ImageNet 图像分类任务上对经典卷积神经网络模型的各种量化方法。结果如下：\n\n总结\n本文提出了 AdaRound，一种对舍入进行优化的后训练量化方法。同样是对网络的权重参数做优化，不同于过去使用深度学习常用的基于全局损失的梯度下降的优化方式，这种方法采用经过特殊设计的优化目标函数来对一部分可学习参数做微调。这种优化方法也被称为基于重建（Reconstruction）的量化方法。TSQ[5]、AdaQuant[6]、Bit Split[7] 等针对模型量化的研究也使用了类似的基于重建的量化方法。优化相比传统的线性量化方法，AdaRound 在最终量化模型的推理精度上有着明显的提升。\nQualcomm AI 研究院在将 AdaRound 方法用在了其全新推出的深度学习模型部署框架 AIMET[8] 中作为核心后训练量化算法，推进了深度模型的轻量化发展与实际场景的应用。\nBRECQ\n\n\nTitle: BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction\nVenue: ICLR 2021\nAuthor(s): Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu\nInstitution(s): University of Electronic Science and Technology of China, SenseTime Research\nLink: arxiv:2102.05426\n\n\n前言\nAdaRound 为后训练量化算法指明了一个很好的方向：利用包含 Hessian 矩阵的二次型来近似量化的全局损失函数。然而，AdaRound 优化方法仍然存在一些局限性：AdaRound 采用层级的量化粒度，忽略了不同层参数之间的相关性，这样会导致在极低比特量化下模型推理精度大幅下降；另外，AdaRound 在将全局损失转化为局部损失时基于两个强假设，忽略了同一层间不同参数的部分相关性和的相关性，这样实际会损失过多有价值的信息。为了摆脱这些局限性，以获得更好的量化效果，本文在 AdaRound 的基础上提出了新的基于块级重建（Block-wise Reconstruction）的优化方法 BRECQ，对上面提到的问题进行了针对性的改进。\n方法\n不同层间参数的相关性\n神经网络的后训练量化一般选择层级的量化粒度，即网络中的每一个参数化层分别对应一个量化映射，这直观上与深度神经网络层级堆叠的设计结构相吻合，但是却忽略了层级之间的参数的相关性。层级量化本质上是对量化这一全局优化问题的简化，显然，神经网络作为一个优化的整体，层级之间参数的相关性必然会对量化效果产生影响。此时仍可以借助 Hessian 矩阵来探究层级之间参数的相关性。\n记神经网络的预激活 $\\mathbf{z} ^{(n)} = f(\\theta)$，任务损失函数为 $L(f(\\theta))$，其中 $\\theta$ 为网络的参数。则损失函数对网络中任意参数的二阶偏导数可以表示为：\n$$\n\\frac{\\partial^{2} L}{\\partial \\theta _{i} \\partial \\theta _{j}} =\n\\frac{\\partial}{\\partial \\theta _{j}} \\left( \\sum _{k=1} ^{m} \\frac{\\partial L}{\\partial \\mathbf{z} _{k}^ {(n)}} \\frac{\\partial \\mathbf{z} _{k} ^{(n)}}{\\partial \\theta _{i}} \\right) =\n\\sum _{k=1} ^{m} \\frac{\\partial L}{\\partial \\mathbf{z} _{k} ^{(n)}} \\frac{\\partial ^{2} \\mathbf{z} _{k} ^{(n)}}{\\partial \\theta _{i} \\partial \\theta _{j}} +\n\\sum _{k,l=1} ^{m} \\frac{\\partial \\mathbf{z} _{k} ^{(n)}}{\\partial \\theta _{i}} \\frac{\\partial ^{2} L}{\\partial \\mathbf{z} _{k} ^{(n)} \\partial \\mathbf{z} _{l} ^{(n)}} \\frac{\\partial \\mathbf{z} _{l} ^{(n)}}{\\partial \\theta _{j}},\n$$\n由于预训练模型的参数可以看作已经收敛，因此可以假设原任务损失 $L$ 对 $\\mathbf{z}$ 的一阶偏导数近似为 $0$，那么，损失函数对任意一组参数的 Hessian 矩阵可以进一步简化为以下的 Guassian-Newton 矩阵形式：\n$$\n\\mathbf{H} ^{(\\theta)} \\approx \\mathbf{G} ^{(\\theta)} = \\mathbf{J} _{\\mathbf{z} ^{(n)}} \\left( \\theta \\right) ^{\\text{T}} \\mathbf{H} ^{\\mathbf{z} ^ {(n)}} \\mathbf{J} _{\\mathbf{z} ^{(n)}} \\left( \\theta \\right),\n$$\n其中 $\\mathbf{J} _{\\mathbf{z} ^{(n)}} (\\theta)$ 是预激活 $\\mathbf{z}$ 对网络参数 $\\theta$ 的 Jacobi 矩阵，$\\mathbf{H} ^{\\mathbf{z} ^ {(n)}}$ 是预激活 $\\mathbf{z}$ 对任务损失 $L$ 的 Hessian 矩阵。这两个矩阵的规模都很大，因此需要对其进行简化。\n简化 Jacobi 矩阵 $\\mathbf{J}$。考虑量化对网络层输出的扰动 $\\Delta \\mathbf{z} ^{(n)}$，Jacobi 矩阵 $\\mathbf{J}$ 可以看作对 $\\Delta \\mathbf{z} ^{(n)}$ 进行一阶泰勒展开后的一阶偏导项：\n$$\n\\Delta \\mathbf{z} ^{(n)} = \\hat{\\mathbf{z}} ^{(n)} - \\mathbf{z} ^{(n)} \\approx \\mathbf{J} _{\\mathbf{z} ^{(n)}} (\\theta) \\Delta \\theta.\n$$\n考虑原量化问题的目标函数，即：\n$$\n\\begin{align*}\n\\underset{\\Delta \\theta}{\\arg\\min} \\space \\Delta \\theta ^{\\text{T}} \\bar{\\mathbf{H}} ^{(\\theta)} \\Delta \\theta\n\\end{align*}\n$$\n将 Jacobi 矩阵 $\\mathbf{J}$ 用泰勒展开的近似式代入，可得简化的量化目标函数：\n$$\n\\begin{align*}\n\\underset{\\hat{\\theta}}{\\arg\\min} \\space \\Delta \\theta ^{\\text{T}} \\bar{\\mathbf{H}} ^{(\\theta)} \\Delta \\theta \\approx \\underset{\\hat{\\theta}}{\\arg\\min} \\space \\mathbb{E} \\left[ \\Delta \\mathbf{z} ^{(n),\\text{T}} \\mathbf{H} ^{\\mathbf{z} ^ {(n)}} \\Delta \\mathbf{z} ^{(n)} \\right].\n\\end{align*}\n$$\n简化 Hessian 矩阵 $\\mathbf{H}$。本文提出使用 Fisher 信息矩阵（Fisher Information Matrix，FIM）来近似预激活 $\\mathbf{z}$ 对网络参数 $\\theta$ 的 Hessian 矩阵。\n给定一个概率模型 $p(x|\\theta)$，使用已知的观测样本通过极大似然估计（Maximum Likelyhood Estimate）对参数 $θ$ 进行优化，使得观测样本出现概率取极大值。相当于使评分函数 $s(θ)$，即对数似然函数对参数 $θ$ 的一阶偏导数趋近 $0$。Fisher 信息定义为该一阶偏导数的二阶矩，在一般情况下，对数似然函数对参数 $θ$ 的一阶偏导数的期望为 $0$，因此 Fisher 信息也可以表示为概率模型对参数极大似然估计的评分函数 $s$ 的方差。对于所有参数 $θ$，FIM 表示如下：\n$$\n\\bar{\\mathbf{F}} ^{(\\theta)} = \\mathbb{E} \\left[ \\nabla _{\\theta} \\log p _{\\theta} (y|x) \\nabla _{\\theta} \\log p _{\\theta} (y|x) ^{\\text{T}} \\right] = - \\mathbb{E} \\left[ \\nabla _{\\theta} ^{2} \\log p _{\\theta} (y|x) \\right] = -\\bar{\\mathbf{H}} _{\\log p(x|\\theta)} ^{(\\theta)}.\n$$\n可以看出，FIM 也可以等价于概率模型的对数似然函数的 Hessian 矩阵的期望的负值。因此，在二阶优化方法中，FIM 一般可以作为 Hessian 矩阵的近似替代。深度神经网络模型的优化方法与概率模型的极大似然估计方法类似，如果能够尽量保证训练样本的分布尽量接近真实数据分布，并且预训练模型参数已经在训练集上收敛，那么就可以使用 FIM 来近似 Hessian 矩阵 $\\mathbf{H}$。\nAdam 优化器使用了参数梯度的二阶矩估计来为不同的参数设计独立的自适应学习率，在计算中使用梯度的平方，即 FIM 的对角元素。本文采用类似的思想，使用了 FIM 的对角元素组成的对角矩阵来替换 Hessian 矩阵。一个直观的解释是，梯度平方的值越大，该参数参与量化重建的重要性越高。替换后的优化目标函数变为：\n$$\n\\underset{\\hat{\\mathbf{w}}}{\\min} \\space \\mathbb{E} \\left[ \\Delta \\mathbf{z} ^{(\\ell), \\text{T}} \\mathbf{H} ^{(\\mathbf{z} ^{(\\ell)})} \\Delta \\mathbf{z} ^{(\\ell)} \\right] =\n\\underset{\\hat{\\mathbf{w}}}{\\min} \\space \\mathbb{E} \\left[ \\Delta \\mathbf{z} ^{(\\ell), \\text{T}} \\text{diag}(\\mathbf{F} ^{(\\mathbf{w})}) \\Delta \\mathbf{z} ^{(\\ell)} \\right].\n$$\n块级重建\n前面提到，基于层级的量化重建会损失不同层参数之间的相关性信息，因此需要探究合适的量化粒度来权衡量化优化问题的规模与层级间相关性损失。本文分析了四种不同的量化重建粒度，从大到小依次为整网（Network-wise）重建、阶段（Stage-wise）重建、块级（Block-wise）重建和层级（Layer-wise）重建，不同粒度的规模如 Figure. 7 所示。\n\n不同的重建粒度，其 Hessian 矩阵的规模也不同。如 Figure. 8 所示，块级或层级的 Hessian 矩阵相当于整网参数的 Hessian 矩阵的分块对角矩阵。\n\n本文对上述四种不同量化粒度的参数重建进行了实验，发现使用块级重建粒度的量化效果最优。使用块级重建的优越性可以从两方面解释。第一，ResNet 等网络采用的残差连接（Residual Connection）可能会加强块级参数的相关性；第二，校准集一般规模较小，要比起粒度更大的阶段或整网重建，块级粒度的参数重建更不容易发生过拟合现象。本文将这种基于块级重建的量化方法称为 BRECQ。\n实验\n消融实验\n本文对上面提到的四种不同的重建粒度进行了消融实验。在 ImageNet 图像分类任务上对 ResNet-18 和 MobileNet-V2 模型进行 2-bit 量化，结果见 Table. 2。结果表明，块级重建的量化效果要比其它粒度的效果要好。\n\n对比实验\n本文在 ImageNet 图像分类任务和 MS COCO 目标检测任务上展开工作。\n在 ImageNet 图像分类任务上的对比结果如下：\n\n在 MS COCO 目标检测任务上的对比结果如下：\n\n总结\n本文提出了 BRECQ，一种由 AdaRound 改进而来的基于块级重建的后训练量化方法。BRECQ 设计了新的优化目标函数，充分考虑了块内参数之间的相关性，弥补了层级重建中层级间的参数相关性的损失。BRECQ 的量化效果要显著优于比 AdaRound 以及其它的基于重建的量化方法。\n商汤科技模型部署与工具链团队在将 BRECQ 方法作为后训练量化算法的示例整合到了他们推出的开源模型量化框架 MQBench[9] 中。\nQDrop\n\n\nTitle: QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization\nVenue: ICLR 2022\nAuthor(s): Xiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, Fengwei Yu\nInstitution(s): State Key Lab of Software Development Environment, Beihang University, SenseTime Research\nLink: arxiv:2203.05740\n\n\n前言\n常规的 PTQ 方法对网络权重和激活的量化是相互独立的，一般的流程是先量化权重值，再量化激活值。AdaRound 和 BRECQ 方法与传统 PTQ 方法类似，它们都是先对网络权重参数进行重建，然后使用传统方法对激活值进行量化。这种 PTQ 流程忽略了激活量化对权重量化的影响，依然存在改进空间。本文提出的 QDrop 在 BRECQ 的基础上进行了改进，将对网络权重的重建和对激活量化的优化结合在一起，充分考虑到了整体的量化损失。QDrop 的量化效果在量化不友好的网络（例如 MobileNet-V2）上显著超越了之前的方法。\n方法\n激活量化如何影响权重量化\n为了验证激活量化对权重参数重建的影响，本文首先基于 BRECQ 方法针对三种不同的激活量化策略开展了对比实验进行观察，三种激活量化策略如下，示意图和结果如 Figure. 9 所示。\n\nCase 1：原始的 BRECQ 方法。重建时不对激活进行量化；\nCase 2：对当前重建的块以及之前的所有重建完毕的块的激活进行量化。\nCase 3：对当前重建的块之前的所有重建完毕的块的激活进行量化，不对当前块的激活进行量化。\n\n\n实验表明，Case 3 的量化效果最好。也就是说，如果在参数重建的过程中对部分块进行激活量化，有可能提高量化效果。\n类似于之前对权重量化的数学定义，对激活的量化也可以定义为对其的扰动，记为 $e=(\\hat{a} - a)$，\n本文将该扰动由减法形式转化为乘法形式，即 $\\hat{a} = a \\cdot (1+u)$。定义同时对权重和激活的量化扰动的全局损失为 $L(\\boldsymbol{w} + \\Delta \\boldsymbol{w}, \\boldsymbol{x}, \\mathbf{1} + \\boldsymbol{u}(\\boldsymbol{x}))$，那么量化要优化的目标函数为：\n$$\n\\min _{\\hat{\\boldsymbol{w}}} \\mathbb{E} _{\\boldsymbol{x}\\sim\\mathcal{D} _{c}} [L(\\boldsymbol{w} + \\Delta \\boldsymbol{w}, \\boldsymbol{x}, \\boldsymbol{1} + \\boldsymbol{u}(\\boldsymbol{x})) - L(\\boldsymbol{w}, \\boldsymbol{x}, \\boldsymbol{1})].\n$$\n卷积、全连接等线性变换层的运算均可以用矩阵乘法 $\\boldsymbol{y}=\\boldsymbol{W}\\boldsymbol{a}$ 来表示。对激活 $\\boldsymbol{a}$ 加入的扰动 $\\boldsymbol{u}(\\boldsymbol{x})$ ，可以传递到对权重 $\\boldsymbol{W}$ 上：\n$$\n\\boldsymbol{W}(\\boldsymbol{a} \\odot\n\\begin{bmatrix}\n1 + \\boldsymbol{u} _{1}(\\boldsymbol{x}) \\\\\n1 + \\boldsymbol{u} _{2}(\\boldsymbol{x}) \\\\\n… \\\\\n1 + \\boldsymbol{u} _{n}(\\boldsymbol{x})\n\\end{bmatrix})\n= (\\boldsymbol{W} \\odot\n\\begin{bmatrix}\n1 + \\boldsymbol{u} _{1}(\\boldsymbol{x}) &amp; 1 + \\boldsymbol{u} _{2} (\\boldsymbol{x}) &amp; … &amp; 1 + \\boldsymbol{u} _{n}(\\boldsymbol{x}) \\\\\n1 + \\boldsymbol{u} _{1}(\\boldsymbol{x}) &amp; 1 + \\boldsymbol{u} _{2} (\\boldsymbol{x}) &amp; … &amp; 1 + \\boldsymbol{u} _{n}(\\boldsymbol{x}) \\\\\n… \\\\\n1 + \\boldsymbol{u} _{1}(\\boldsymbol{x}) &amp; 1 + \\boldsymbol{u} _{2} (\\boldsymbol{x}) &amp; … &amp; 1 + \\boldsymbol{u} _{n}(\\boldsymbol{x})\n\\end{bmatrix})\n\\boldsymbol{a}.\n$$\n目标函数可以进一步转化为：\n$$\n\\mathbb{E} _{\\boldsymbol{x}\\sim\\mathcal{D} _{c}} [L (\\hat{\\boldsymbol{w}}, \\boldsymbol{x}, \\mathbf{1} + \\boldsymbol{u}(\\boldsymbol{x})) - L(\\boldsymbol{w}, \\boldsymbol{x}, \\boldsymbol{1})] \\approx \\mathbb{E} _{\\boldsymbol{x}\\sim\\mathcal{D} _{c}} [L(\\hat{\\boldsymbol{w}} \\odot(\\mathbf{1} + \\boldsymbol{v}(\\boldsymbol{x})), \\boldsymbol{x}, \\boldsymbol{1}) - L(\\boldsymbol{w}, \\boldsymbol{x}, \\boldsymbol{1})],\n$$\n$$\n\\begin{align*}\n\\mathbb{E} _{\\boldsymbol{x} \\sim \\mathcal{D} _{c}} [L(\\hat{\\boldsymbol{w}}, \\boldsymbol{x}, \\boldsymbol{1} + \\boldsymbol{u}(\\boldsymbol{x})) - L(\\boldsymbol{w}, \\boldsymbol{x}, \\boldsymbol{1})] &amp;\\approx \\\\\n\\mathbb{E} _{\\boldsymbol{x} \\sim \\mathcal{D} _{c}} [\\underbrace{(L(\\hat{\\boldsymbol{w}}, \\boldsymbol{x}, \\boldsymbol{1}) - L(\\boldsymbol{w},\\boldsymbol{x}, \\boldsymbol{1}))} _{(7-1)} &amp;+ \\underbrace{(L(\\hat{\\boldsymbol{w}} \\odot (\\boldsymbol{1} + \\boldsymbol{v}(\\boldsymbol{x})), \\boldsymbol{x}, \\boldsymbol{1}) - L(\\hat{\\boldsymbol{w}}, \\boldsymbol{x}, \\boldsymbol{1}))} _{(7-2)}]\n\\end{align*}\n$$\n上式的第一项为权重参数重建的损失，第二项为参数重建后的网络再加入激活量化之后的损失。也就是说，通过乘法项引入的对激活的量化扰动，对整体的量化损失是有影响的。\n可以从量化扰动鲁棒性的角度来理解由激活引入的第二项损失。对激活的量化可以看作对参数重建后的量化网络加入的噪声。如果在输入加入了噪声的情况下进行优化，网络参数优化时的损失平面理论上会更加平坦，即鲁棒性更强。这也解释了前面对比实验中 Case 2 和 Case 3 比第一种情况的效果更好的原因。\nQDrop\n本文提出了一种权重和激活量化相结合的参数重建优化方案，命名为 QDrop。QDrop 是从前文所述的对比实验的第三种情况加以改进得到的。与原先不同的是，QDrop 对当前重建的层也加入了对输入激活的量化，但是只会以一定的概率 $p$ 来决定输入的每一个元素是否不进行量化，类似于深度学习中的 Dropout，即：\n$$\n\\mathrm{QDROP}:u=\n\\begin{cases}\n0 &amp; \\text{with probability }p \\\\\n\\frac{\\hat{a}}{a}-1 &amp; \\text{with probability }1-p &amp;\n\\end{cases}.\n$$\n使用 QDrop 得到的参数优化损失平面与其它情况的对比如 Figure. 10 所示。可以看到，使用 QDrop 方法的损失平面要比之前更加平坦。\n\n实验\n消融实验\nQDrop 与 No Drop。本文对比了使用 QDrop 方法进行随机地激活量化和完全进行激活量化的效果。结果如表所示。结果表明使用 QDrop 后预测精度有所提升。\n\nQDrop 概率 $p$ 的选择。本文对 QDrop 概率 $p$ 的选择进行了消融实验，结果如图所示。结果表明，QDrop 概率 $p$ 的值对量化效果有较大影响，过小或过大的概率都会导致量化效果下降。\n可以看出，当 $p=0.5$ 时量化模型的预测精度最高。但对于更细粒度的搜索空间本文未作过多讨论。\n\n对比实验\nQDrop 在 ImageNet 图像分类任务、MS COCO 目标检测任务和各个自然语言处理任务上展开工作。\n在 ImageNet 图像分类任务上的对比结果如下：\n\n在 MS COCO 目标检测任务上的对比结果如下：\n\n在各个自然语言处理任务上的对比结果如下：\n\n总结\n本文提出了 QDrop，一种由 BRECQ 改进而来的基于块级重建的后训练量化方法。基于重建的优化方法相比过去的 PTQ 方法的一个优势在于，激活量化和权重量化可以同时进行。QDrop 将对激活的量化与对权重的重建结合到了一起，充分考虑了量化任务对网络模型的整体影响，最终的量化效果达到了 PTQ 方法的新极限。\n商汤科技模型部署与工具链团队在 ICLR 2022 发表相关研究论文之后，将 QDrop 方法作为后训练量化算法的示例整合到了他们推出的开源模型量化框架 MQBench 中，并将在之后陆续支持对 NLP 相关任务的网络模型的量化。\n参考文献\n\n\n\nZhewei Yao, Amir Gholami, Kurt Keutzer, et al. PyHessian: Neural Networks Through the Lens of the Hessian. BigData 2020. ↩︎\n\nMarkus Nagel, Rana Ali Amjad, Mart van Baalen, et al. Up or Down? Adaptive Rounding for Post-Training Quantization. ICML, 2020. ↩︎\n\nYuhang Li, Ruihao Gong, Xu Tan, et al. BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction. ICLR, 2021. ↩︎\n\nXiuying Wei, Ruihao Gong, Yuhang Li, et al. QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization. ICLR, 2022. ↩︎\n\nPeisong Wang, Qinghao Hu, Yifan Zhang, et al. Two-Step Quantization for Low-bit Neural Networks. CVPR 2018. ↩︎\n\nItay Hubara, Yury Nahshan, Yair Hanani, et al. Improving Post Training Neural Quantization: Layer-wise Calibration and Integer Programming. ICML, 2021. ↩︎\n\nPeisong Wang, Qiang Chen, Xiangyu He, et al. Towards Accurate Post-training Network Quantization via Bit-Splitting and Stitching. ICML, 2020. ↩︎\n\nSangeetha Siddegowda, Marios Fournarakis, Markus Nagel, et al. Neural Network Quantization with AI Model Efficiency Toolkit (AIMET). ArXiv, 2022, abs/2201.08442. ↩︎\n\nYuhang Li, Mingzhu Shen, Yan Ren, et al. MQBench: Towards Reproducible and Deployable Model Quantization Benchmark. NeurIPS Track on Datasets and Benchmarks, 2021. ↩︎\n\n\n\n","categories":["Paper Reading"],"tags":["Deep Learning","Post-Training Quantization","Hessian Matrix"]}]